<!--<!doctype html>
<html>-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<head>
 <link rel="Shortcut Icon" href="./logo/hp_logo.jpg" sizes=16x16  type="image/x-icon" />
 <link rel="Bookmark" href="./logo/hp_logo.jpg" sizes=16x16 type="image/x-icon" />
<!--<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">-->
  <title>Chongyi Li</title>
	<style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
.title { width : 650px; margin : 20px auto; }
.container { width : 700px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 40px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 10px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 0px; font-weight: normal;}
.publication strong a { color : #0000A0; }
.publication .links { position :relative ; top : 10px }
.publication .links a { margin-right : 5px; font-size: 15px; font-weight: normal}
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
li, ul {font-weight: normal;}
</style>
<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<meta name="viewport" content="width=device-width">
<script async="" src="./analytics.js"></script>
</head>
<body>
<div class="wrapper">
<header>
<h7>Chongyi Li</h7><br><br>
<div>
<img src="sub_img/lichongyi_nankai.jpg" border="0" width="90%"><br></div><br>

  
<p>
<small>üìç Nankai, Tianjin, China</small><br>
<small>üìßlichongyi25 at gmail.com</small><br>
<small>üìßlichongyi at nankai.edu.cn</small><br>
	
<!--<a href="https://github.com/Li-Chongyi/" target="_blank">[GitHub]</a>-->  
<!--<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chongyi" target="_blank">[DBLP]</a>  <br>-->
<a href="https://scholar.google.com/citations?user=1_I0P-AAAAAJ&hl=en" target="_blank">[Google Scholar]</a> <br>
<a href="https://pi-lab.xyz/index.html" target="_blank">[&pi; Research Group]</a>
</p> <br>
	
	
<p class="view"><a href="https://li-chongyi.github.io/">Homepage</a></p>
<p class="view"><a href="sub_publication.html">Publications</a></p>
<!--<p class="view"><a href="datasets.html">Datasets</a></p>-->

	
<!--<p class="view"><a href="sub_projects.html">Projects</a></p>-->
</header>

<section>

<h2>
<a id="Biography-page" class="anchor" href="#biography-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome to Chongyi Li (ÊùéÈáç‰ª™)'s Homepage</h2>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>
<br>I am a  Professor at Nankai University, China. I was a Research Assistant Professor at NTU and  was also a research fellow at NTU and CityU. I was a joint Ph.D. student at ANU and TJU.

<hr />
</p>




<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Open Positions:</h2>
<br><p><font color="red">I am looking for several Postdoctoral Fellows at Nankai University (work in Tianjin or Shenzhen). The application deadline is 11 March 2024. If you wanna have a try, please drop me an email. </font></p></br>
</ul>
<br>	
<br><p><font color="blue">I am looking for PhD students and Master students who want to conduct
research and develop advanced deep learning algorithms for image and video enhancement and restoration, computational imaging, and image signal processor to join my research group at Nankai University (2023 Fall or 2024 Fall). I am also recruiting Research Associates and Final Year Project students.</font></p></br>
</ul>
<br>	
-->
<!--
<hr /> 
</p>
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Research Interests:</h2>

<ul>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>My primary reseach interests include artificial intelligence, machine learning, computer vision, and image processing, particularly in the domains of 
  <li><strong>Image and Video Restoration and Enhancement</strong></li> <br/>
	The purpose is to develop algorithms to process an image or video so that result is more suitable than original image or video for specific application. The specific research topics are 
  <ol type="a" start="1">
      <li>restoring and enhancing the images and videos captured in adverse weather (hazy, foggy, sandy, dusty, rainy, snowy day)</li>
      <li>restoring and enhancing the images and videos captured in special circumstances or devices (underwater, weak illumination, dark, under-display devices)</li>
      <li>general photo enhancement, auto image retouching</li>
      <li>image/depth super-resolution, image deblurring, image denosing</li>
  </ol>
  <li><strong>Multi-Modality Scene Understanding</strong> </li><br/>
	  The purpose is to design AI models to perceive and understand scenes. The specific research topics are
  <ol type="a" start="1">
      <li>RGB-D salient object detection</li>
      <li>co-salient object detection</li>
      <li>remote sensing image salient object detection</li>
  </ol>  

  
</ul>
<br>
<hr />
</p>
-->
<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recent News:</h2>
 
<ul>
<li> 2023/10 --3 papers (including one spotlight paper) got accepted by <strong>NeurIPS 2023</strong>.</li>
<li> 2023/09 --Two papers have been recognized as <strong>ESI Hot Paper</strong></li> 

</ul>
<br>
<hr />

-->

<div class="container">
<h2><a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Selected Publications:</h2>
        <br>

	
	<div class="publication">
          <img src="logo//InclusionMatching.gif" onmouseover="this.src='logo//InclusionMatching.gif';" onmouseout="this.src='logo/InclusionMatching.gif';" class="publogo"  width="300 px">
	<p>     
	
                <strong>
                    <a href="">Learning Inclusion Matching for Animation Paint Bucket Colorization</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2024 <a href="" target="_blank"><font color="#ff0000"></font></a></b></em>
                <br> 
              Yuekun Dai, Shangchen Zhou, Qinyue Li, Chongyi Li, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2403.18342">PDF</a>| 
		    <a href="https://ykdai.github.io/projects/InclusionMatching/">Project Page</a>| 	
                    <a href="https://github.com/ykdai/BasicPBC/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />
	
	
	<div class="publication">
         <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/backlit_demo.mp4"> <source src="logo/backlit_demo.mp4" type="video/mp4"></video>    
	<p>     
	
                <strong>
                    <a href="">Iterative Prompt Learning for Unsupervised Backlit Image Enhancement</a>
                </strong>
		  <br> 
		<em><b>ICCV, 2023 <a href="" target="_blank"><font color="#ff0000"></font></a></b></em>
                <br> 
              Zhexin Liang, Chongyi Li, Shangchen Zhou, Ruicheng Feng, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2303.17569">PDF</a>| 
		    <a href="https://zhexinliang.github.io/CLIP_LIT_page/">Project Page</a>| 	
                    <a href="https://zhexinliang.github.io/CLIP_LIT_page/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />

	<div class="publication">
         <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/ProPainter_demo.mp4"> <source src="logo/ProPainter_demo.mp4" type="video/mp4"></video>    
	<p>     
	
                <strong>
                    <a href="">ProPainter: Improving Video Inpainting with Enhanced Propagation and Efficient Transformer</a>
                </strong>
		  <br> 
		<em><b>ICCV, 2023 </b></em>
                <br> 
              Shangchen Zhou, Chongyi Li, Kelvin C.K. Chan, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2309.03897">PDF</a>| 
		    <a href="https://shangchenzhou.com/projects/ProPainter/">Project Page</a>| 	
                    <a href="https://github.com/sczhou/ProPainter">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />


	   <div class="publication">
            <img src="logo//LED_video_demo.gif" onmouseover="this.src='logo//LED_video_demo.gif';" onmouseout="this.src='logo/LED_video_demo.gif';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising</a>
                </strong>
		  <br> 
		<em><b>ICCV, 2023 </b></em>
                <br> 
              Xin Jin, Jiawen Xiao, Linghao Han, Chunle Guo, Ruixuan Zhang, Xialei Liu, and Chongyi Li.
                <br>
                <span class="links">
                    <a href="https://srameo.github.io/projects/led-iccv23/assets/paper/1819_arxiv_v1_more_visual_results.pdf">PDF</a>| 
		    <a href="https://srameo.github.io/projects/led-iccv23/">Project Page</a>| 	
                    <a href="https://github.com/Srameo/LED">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <be>
	 <br>
	<br />
	<br />
	<br />
	<br />


	
	   <div class="publication">
            <img src="logo/BracketFlare_logo.png" onmouseover="this.src='logo/BracketFlare_logo.png';" onmouseout="this.src='logo/BracketFlare_logo.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2023 </b></em>
                <br> 
              Yuekun Dai, Yihang Luo, Shangchen Zhou, Chongyi Li, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_Nighttime_Smartphone_Reflective_Flare_Removal_Using_Optical_Center_Symmetry_Prior_CVPR_2023_paper.pdf">PDF</a>| 
		    <a href="https://ykdai.github.io/projects/BracketFlare">Project Page</a>| 	
                    <a href="https://github.com/ykdai/BracketFlare">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br>
	<br />
	<br />
	<br />
	
	
	   <div class="publication">
            <img src="logo/DNF_logo.png" onmouseover="this.src='logo/DNF_logo.png';" onmouseout="this.src='logo/DNF_logo.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">DNF: Decouple and Feedback Network for Seeing in the Dark</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2023 </b></em>
                <br> 
              Xin Jin,  Linghao Han, Zhen Li, Zhi Chai, Chunle Guo, and Chongyi Li.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_DNF_Decouple_and_Feedback_Network_for_Seeing_in_the_Dark_CVPR_2023_paper.pdf">PDF</a>| 
		    <a href="https://github.com/Srameo/DNF">Project Page</a>| 	
                    <a href="https://github.com/Srameo/DNF">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	<div class="publication">
         <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/RIDCP_video.mp4"> <source src="logo/RIDCP_video.mp4" type="video/mp4"></video>    
	<p>     
	
                <strong>
                    <a href="">RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2023 </b></em>
                <br> 
              Ruiqi Wu,  Zhengpeng Duan, Chunle Guo, Zhi Chai, and Chongyi Li.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_RIDCP_Revitalizing_Real_Image_Dehazing_via_High-Quality_Codebook_Priors_CVPR_2023_paper.pdf">PDF</a>| 
		    <a href="https://rq-wu.github.io/projects/RIDCP/index.html">Project Page</a>| 	
                    <a href="https://github.com/RQ-Wu/RIDCP_dehazing">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />
	
	
		

	

	
	<div class="publication">
            <img src="logo/SKF_logo.png" onmouseover="this.src='logo/SKF_logo.png';" onmouseout="this.src='logo/SKF_logo.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2023</b></em>
                <br> 
              Yuhui Wu, Chen Pan, Guoqing Wang, Yang Yang, Jiwei Wei, Chongyi Li, and Hengtao Shen.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Learning_Semantic-Aware_Knowledge_Guidance_for_Low-Light_Image_Enhancement_CVPR_2023_paper.pdf">PDF</a>| 
		    <a href="https://github.com/langmanbusi/Semantic-Aware-Low-Light-Image-Enhancement">Project Page</a>| 	
                    <a href="https://github.com/langmanbusi/Semantic-Aware-Low-Light-Image-Enhancement">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />
	
		<div class="publication">
            <img src="logo/AlignFormer_logo.png" onmouseover="this.src='logo/AlignFormer_logo.png';" onmouseout="this.src='logo/AlignFormer_logo.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2023</b></em>
                <br> 
             Ruicheng Feng, Chongyi Li, Huaijin Chen, Shuai Li, Jinwei Gu, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Generating_Aligned_Pseudo-Supervision_From_Non-Aligned_Data_for_Image_Restoration_in_CVPR_2023_paper.pdf">PDF</a>| 
		    <a href="https://jnjaby.github.io/projects/AlignFormer/">Project Page</a>| 	
                    <a href="https://github.com/jnjaby/AlignFormer">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br>
	<br />
	<br />
	<br />
	
		  <div class="publication">
            <img src="logo/UDHFour.png" onmouseover="this.src='logo/UDHFour.png';" onmouseout="this.src='logo/UDHFour.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement</a>
                </strong>
		  <br> 
		<em><b>ICLR, 2023 </b></em>
                <br> 
              Chongyi Li, Chunle Guo, Man Zhou, Zhexin Liang, Shangchen Zhou, Ruicheng Feng, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2302.11831">PDF</a>| 
		    <a href="https://li-chongyi.github.io/UHDFour/">Project Page</a>| 	
                    <a href="https://github.com/Li-Chongyi/UHDFour_code">Code</a>|
		    <a href="https://drive.google.com/drive/folders/1IneTwBsSiSSVXGoXQ9_hE1cO2d4Fd4DN?usp=share_link">Dataset</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />
	

      <div class="publication">
            <img src="logo/URanker.png" onmouseover="this.src='logo/URanker.png';" onmouseout="this.src='logo/URanker.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Underwater Ranker: Learn Which Is Better and How to Be Better</a>
                </strong>
		  <br> 
		<em><b>AAAI, 2023 </b></em>
                <br> 
              Chunle Guo, Ruiqi Wu, Xin Jin, Linghao Han, Zhi Chai, Weidong Zhang, and Chongyi Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2208.06857.pdf">PDF</a>| 
		    <a href="https://li-chongyi.github.io/URanker_files/">Project Page</a>| 	
                    <a href="https://github.com/RQ-Wu/UnderwaterRanker">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	  <div class="publication">
            <img src="logo/Cudi.jpg" onmouseover="this.src='logo/Cudi.jpg';" onmouseout="this.src='logo/Cudi.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment</a>
                </strong>
		  <br> 
		<em><b>2023</b></em>
                <br> 
              Chongyi Li, Chunle Guo, Ruicheng Feng, Shangchen Zhou, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2207.14273.pdf">PDF</a>| 
		    <a href="https://li-chongyi.github.io/CuDi_files/">Project Page</a>| 	
                    <a href="https://li-chongyi.github.io/CuDi_files/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />


<!--	
	
	<div class="publication">
 <img src="logo/Beauty_REC_dataset.png" onmouseover="this.src='logo/Beauty_REC_dataset.png';" onmouseout="this.src='logo/Beauty_REC_dataset.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">BeautyREC: Robust, Efficient, and Content-preserving Makeup Transfer</a>
                </strong>
		  <br> 
		<em><b>NTIRE CVPRW, 2023 <a href="" target="_blank"><font color="#ff0000">[Oral]</font></a></b></em>
                <br> 
              Qixin Yan, Chunle Guo, Jixin Zhao, Yuekun Dai, Chen Change Loy, and <b>Chongyi Li<sup>+</sup></b>.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2212.05855">PDF</a>| 
		    <a href="https://li-chongyi.github.io/BeautyREC_files/">Project Page</a>| 	
                    <a href="https://github.com/learningyan/BeautyREC/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />

		<div class="publication">
 <img src="logo/FlexiCurve.png" onmouseover="this.src='logo/FlexiCurve.png';" onmouseout="this.src='logo/FlexiCurve.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">FlexiCurve: Flexible Piecewise Curves Estimation for Photo Retouching</a>
                </strong>
		  <br> 
		<em><b>NTIRE CVPRW, 2023 <a href="" target="_blank"><font color="#ff0000">[Oral]</font></a></b></em>
                <br> 
              <b>Chongyi Li</b>, Chunle Guo, Shangchen Zhou, Qiming Ai, Ruicheng Feng, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_FlexiCurve_Flexible_Piecewise_Curves_Estimation_for_Photo_Retouching_CVPRW_2023_paper.pdf">PDF</a>| 
		    <a href="https://li-chongyi.github.io/FlexiCurve">Project Page</a>| 	
                    <a href="https://li-chongyi.github.io/FlexiCurve">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br>
	 <br>
	<br />
	<br />
	<br />
	<br />
		
-->	
	
	  <div class="publication">
            <!--<img src="logo/codeformer1.jpg" onmouseover="this.src='logo/codeformer2.jpg';" onmouseout="this.src='logo/codeformer1.jpg';" class="publogo"  width="300 px">-->
         <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/Coderformer_video2.mp4"> <source src="logo/Coderformer_video2.mp4" type="video/mp4"></video>    
	<p> 
                <strong>
                    <a href="">Towards Robust Blind Face Restoration with Codebook Lookup Transformer</a>
                </strong>
		  <br> 
		<em><b>NeurIPS, 2022</b></em>
                <br> 
                Shangchen Zhou, Kelvin C. K. Chan, Chongyi Li, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2206.11253.pdf">PDF</a>| 
		    <a href="https://shangchenzhou.com/projects/CodeFormer/">Project Page</a>| 	
		    <a href="https://github.com/sczhou/CodeFormer">Code</a>| 
		    <a href="https://www.youtube.com/watch?v=d3VDpkXlueI">Video</a>| 
		    <a href="https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing">Colab</a>
                </span>
            </p>
          </div>
          <br>
	<br>
       <br />
	<br />
	
	<div class="publication">
            <img src="logo/Continuous_Fourier_transform_of_rect_and_sinc_functions.gif" onmouseover="this.src='logo/Continuous_Fourier_transform_of_rect_and_sinc_functions.gif';" onmouseout="this.src='logo/Continuous_Fourier_transform_of_rect_and_sinc_functions.gif';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Deep Fourier Up-Sampling</a>
                </strong>
		  <br> 
		<em><b>NeurIPS, 2022 </b></em>
                <br> 
              Man Zhou, Hu Yu, Jie Huang, Feng Zhao, Jinwei Gu, Chen Change Loy, Deyu Meng, and Chongyi Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2210.05171">PDF</a>| 
		    <a href="https://li-chongyi.github.io/FourierUp_files/">Project Page</a>| 	
                    <a href="https://github.com/manman1995/Deep-Fourier-Upsampling">Code</a>
                </span>
            </p>
          </div>
          <br>
	<br />

	  <div class="publication">
            <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/7K_flare.mp4"> <source src="logo/7K_flare.mp4" type="video/mp4"></video>    
            <p> 
                <strong>
                    <a href="">Flare7K: A Phenomenological Nighttime Flare Removal Dataset</a>
                </strong>
		  <br> 
		<em><b>NeurIPS, 2022</b></em>
                <br> 
               Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2210.06570">PDF</a>| 
		    <a href="https://nukaliad.github.io/projects/Flare7K">Project Page</a>| 	
		    <a href="https://drive.google.com/file/d/1PPXWxn7gYvqwHX301SuWmjI7IUUtqxab/view">Dataset</a>|
		  <a href="https://www.youtube.com/watch?v=CR3VFj4NOQM&feature=youtu.be">Video</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br>
	<br />
	<br />
	<br />
	

	


	 <div class="publication">
            <!--<img src="logo/LEDNet_real_input1.jpg" onmouseover="this.src='logo/LEDNet_real_output1.jpg';" onmouseout="this.src='logo/LEDNet_real_input1.jpg';" class="publogo"  width="300 px">-->
        <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/LEDNet_video_demo.mp4"> <source src="logo/LEDNet_video_demo.mp4" type="video/mp4"></video>        
	<p> 
                <strong>
                    <a href="">LEDNet: Joint Low-light Enhancement and Deblurring in the Dark</a>
                </strong>
		  <br> 
		<em><b>ECCV, 2022</b></em>
                <br> 
               Shangchen Zhou, Chongyi Li, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2202.03373.pdf">PDF</a>| 
		    <a href="https://shangchenzhou.com/projects/LEDNet/">Project Page</a>| 	
                    <a href="https://github.com/sczhou/LEDNet/">Code</a>|
		    <a href="https://drive.google.com/drive/folders/11HcsiHNvM7JUlbuHIniREdQ2peDUhtwX//">Dataset</a>|
	            <a href="https://replicate.com/sczhou/lednet/">Replicate</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	



	 <div class="publication">
            <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/HLRP-8M.mp4"> <source src="logo/HLRP-8M.mp4" type="video/mp4"></video>
            <p> 
                <strong>
                    <a href="">Underwater Image Enhancement with Hyper-Laplacian Reflectance Priors</a>
                </strong>
		  <br> 
		<em><b>TIP, 2022 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper & ESI Hot Paper & TIP Popular Articles]</font></a></b></em>
                <br> 
               Peixian Zhuang, Jiamin Wu, Fatih Porikli, and Chongyi Li.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9854113">PDF</a>| 
		    <a href="https://github.com/zhuangpeixian/HLRP">Project Page</a>| 	
                    <a href="https://github.com/zhuangpeixian/HLRP">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />


	 <div class="publication">
            <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="MLLE_files/videos/underwater_video_demo.mp4"> <source src="MLLE_files/videos/underwater_video_demo.mp4" type="video/mp4"></video>
            <p> 
                <strong>
                    <a href="">Underwater Image Enhancement via Minimal Color Loss and Locally Adaptive Contrast Enhancement</a>
                </strong>
		  <br> 
		<em><b>TIP, 2022<a  href="" target="_blank"><font color="#ff0000"> [ESI Highly Cited Paper & ESI Hot Paper]</font></a></b></em>
                <br> 
               Weidong Zhang, Peixian Zhuang, Haihan Sun, Guohou Li, Sam Kwong, and Chongyi Li.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9788535">PDF</a>| 
		    <a href="https://li-chongyi.github.io/proj_MMLE">Project Page</a>| 	
                    <a href="https://github.com/Li-Chongyi/MMLE_code">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />

<div class="publication">
           <img src="logo/CIR-Net_logo.JPG" onmouseover="this.src='logo/CIR-Net_logo.JPG';" onmouseout="this.src='logo/CIR-Net_logo.JPG';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">CIR-Net: Cross-modality Interaction and Refinement for RGB-D Salient Object Detection</a>
                </strong>
		  <br> 
		<em><b>TIP, 2022</b></em>
                <br> 
              Runmin Cong, Qinwei Lin, Chen Zhang, Chongyi Li, Xiaochun Cao, Qingming Huang, and Yao Zhao.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2210.02843">PDF</a>| 
		    <a href="https://rmcong.github.io/proj_CIRNet.html">Project Page</a>| 	
                    <a href="https://github.com/rmcong/CIRNet_TIP2022">Code</a>	
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
		<!-- 
<div class="publication">
           <img src="logo/TCYB2022 _input.jpg" onmouseover="this.src='logo/TCYB2022_output.jpg';" onmouseout="this.src='logo/TCYB2022 _input.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Global-and-Local Collaborative Learning for Co-Salient Object Detection</a>
                </strong>
		  <br> 
		<em><b>TCYB, 2022</b></em>
                <br> 
              Runmin Cong, Ning Yang, Chongyi Li, Huazhu Fu, Yao Zhao, Qingming Huang, and Sam Kwong.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2204.08917.pdf">PDF</a>| 
		    <a href="https://rmcong.github.io/proj_GLNet.html/">Project Page</a>| 	
                    <a href="https://github.com/rmcong/GLNet_TCYB2022">Code</a>	
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	-->


		 
	<div class="publication">
            <a href="https://imgsli.com/MTI4ODIz"><img src="logo/UDC.jpg" onmouseover="this.src='logo/UDC.jpg';" onmouseout="this.src='logo/UDC.jpg';" class="publogo"  width="300 px"></a>
            <p> 
                <strong>
                    <a href="">Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network</a>
                </strong>
		 <br> 
		<em><b>CVPR, 2021</b></em>
                <br> 
               Ruicheng Feng, Chongyi Li, Huaijin Chen, Shuai Li, Chen Change Loy, Jinwei Gu.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2104.09556">PDF</a>| 
                    <a href="https://jnjaby.github.io/projects/UDC/">Project Page</a>| 
                    <a href="https://github.com/jnjaby/DISCNet">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
		 
	  <div class="publication">
            <img src="./logo/survey.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Low-Light Image and Video Enhancement Using Deep Learning: A Survey</a>
                </strong>
		  <br> 
		<em><b>TPAMI, 2021 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper & ESI Hot Paper]</font></a></b></em>
                <br> 
               Chongyi Li, Chunle Guo, Linghao Han, Jun Jiang, Ming-Ming Cheng, Jinwei Gu, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2104.10729">PDF</a>| 
		    <a href="https://www.mmlab-ntu.com/project/lliv_survey/index.html">Project Page</a>| 	
                    <a href="https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open/">Collection</a>|
		    <a href="https://drive.google.com/file/d/1QS4FgT5aTQNYy-eHZ_A89rLoZgx_iysR/view?usp=sharing/">Dataset</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	

	
	  <div class="publication">
            <img src="./logo/zerodce++.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/9369102">Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation</a>
                </strong>
		  <br>
		<em><b>TPAMI, 2021 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper & TPAMI Popular Articles]</font></a></b></em>
                <br>
               Chongyi Li, Chunle Guo, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9369102">PDF</a>| 
                    <a href="https://li-chongyi.github.io/Proj_Zero-DCE++.html">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/Zero-DCE_extension">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	


	
        <div class="publication">
            <img src="./logo/ECCV20-visual_results.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</a>
                </strong>
		  <br>
		<em><b>ECCV, 2020</b></em>
                <br>
                Chongyi Li, Runmin Cong, Yongri Piao, Qianqian Xu, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222.pdf">PDF</a>| 
                    <a href="https://li-chongyi.github.io/Proj_ECCV20">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/cmMS-ECCV20">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/ACMMM_framework.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://dl.acm.org/doi/10.1145/3394171.3413928">NuI-Go: Recursive Non-Local Encoder-Decoder Network for Retinal Image Non-Uniform Illumination Removal</a>
                </strong>
		   <br>
		<em><b>ACM MM, 2020</b></em>
                <br>
                Chongyi Li, Huazhu Fu, Runmin Cong, Zechao Li, Qianqian Xu.
                <br>
                <span class="links">
                    <a href="https://dl.acm.org/doi/10.1145/3394171.3413928">PDF</a>| 
                    <a href="Proj_ACMMM20_NuI-Go.html">Project Page</a>| 
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	

	<!--
        <div class="publication">
            <img src="./logo/tgars_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8793227">Nested Network With Two-Stream Pyramid for Salient Object Detection in Optical Remote Sensing Images</a>
                </strong>
		     <br>
		 <em><b>TGRS, 2020 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper]</font></a></b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Junhui Hou, Sanyi Zhang, Yue Qian, Sam Kwong.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8793227">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_optical_saliency.html">Project Page</a>| 
		    <a href="https://drive.google.com/file/d/15FOnRo1xnz05fcNkXBhWy8WL0C26i8y4/view">Benchmark</a>| 
                    <a href="https://drive.google.com/file/d/1nnZKphu9_4oBvie4yqdxG95tYZRsqj4W/view">Results</a>
                </span>
           </p>
        </div>
        <br>
        <br>
	<br />
	<br />
	-->
        <div class="publication">
            <img src="./logo/PR_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://www.sciencedirect.com.remotexs.ntu.edu.sg/science/article/pii/S0031320319303401">Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement</a>
                </strong>
		     <br>
		 <em><b>PR, 2020 <a href="./PDF/PR_honorable mention_2020.pdf" target="_blank"><font color="#ff0000">[Pattern Recognition Best Paper Honourable Mention & ESI Highly Cited Paper]</font></a></b></em>
                <br>
                Chongyi Li, Saeed Anwar, Fatih Porikli.
                <br>
                <span class="links">
                    <a href="https://www.sciencedirect.com.remotexs.ntu.edu.sg/science/article/pii/S0031320319303401">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html">Project Page</a>| 
                    <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html">Code</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
	<!--
        <div class="publication">
            <img src="./logo/TC2020_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/stamp/stamp.jsp?tp=&arnumber=8998588">ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D Salient Object Detection</a>
                </strong>
		     <br>
		<em><b>TCYB, 2020 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper & TCYB Popular Articles]</font></a></b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Sam Kwong, et al.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/stamp/stamp.jsp?tp=&arnumber=8998588">PDF</a>| 
                    <a href="https://github.com/Li-Chongyi/ASIF-Net">Code and Results</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
	-->


	
        <div class="publication">
            <img src="./logo/depth_SR_logo.png" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8579111">Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution</a>
                </strong>
		     <br>
		<em><b>TIP, 2019</b></em>
                <br>
                Chunle Guo, Chongyi Li, Jichang Guo, Runmin Cong, Huazhu Fu, Ping Han.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8579111">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_SR.html">Project Page</a>| 
                    <a href="https://drive.google.com/file/d/18y6jpGnjqYINzMFJeVHcg4SQiYM-lFBb/view">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br />
	 <br />	

	
        <div class="publication">
            <img src="./logo/TIP16_logo.png" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/7574330">Underwater Image Enhancement by Dehazing with Minimum Information Loss and Histogram Distribution Prior</a>
                </strong>
		     <br>
		 <em><b>TIP, 2016 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper & TIP Popular Articles]</font></a></b></em>
                <br>
                Chongyi Li, Jichang Guo, Runmin Cong, et al.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/7574330">PDF</a>| 
                    <a href="">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/TIP2016-code">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
</div>
<br>
<hr />





<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Events:</h2>  

<div class="publication">
           <img src="./logo/MIPI_2023.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Mobile Intelligent Photography & Imaging</a>
                </strong>
		  <br> 
		<em><b>2nd MIPI workshop @ CVPR 2023</b></em>
                <br> 
              <b>Chongyi Li</b>, Shangchen Zhou, Ruicheng Feng,  Yuekun Dai, Qingpeng Zhu, Qianhui Sun,  Wenxiu Sun,  Chen Change Loy, and Jinwei Gu.
                <br>
                <span class="links">
		      <a href="https://mipi-challenge.org/MIPI2023/">Website</a>	
		      
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />

<div class="publication">
            <img src="logo/codeformer1.jpg" onmouseover="this.src='logo/codeformer2.jpg';" onmouseout="this.src='logo/codeformer1.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Try Your Face (face restoration platform)</a>
                </strong> 
              We provide our code of CodeFormer (Towards Robust Blind Face Restoration with Codebook Lookup TransFormer) on Colab. Please feel free to try your face.
                <br>
                <span class="links">
		      <a href="https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing">Colab</a>	
		      
                </span>
            </p>
          </div>
          <br>
	  <br>
	<br />
	<br />
	<br />
	
<div class="publication">
            <img src="logo/plateform1.jpg" onmouseover="this.src='logo/plateform2.jpg';" onmouseout="this.src='logo/plateform1.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Low-Light Image Enhancement Online Platform</a>
                </strong> 
              Different algorithms demand various configurations, GPU versions, and hardware specifications that are prohibitive to beginners who are new to this area and may not even have GPU resources. We contribute an online plateform.
                <br>
                <span class="links">
		      <a href="http://mc.nankai.edu.cn/ll/">Website</a>	
		      
                </span>
            </p>
          </div>
          <br>
	  <br>
	<br />
	<br />
	<br />
	
	  <div class="publication">
           <img src="./logo/MIPI.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Mobile Intelligent Photography & Imaging</a>
                </strong>
		  <br> 
		<em><b>1st MIPI workshop @ ECCV 2022</b></em>
                <br> 
              <b>Chongyi Li</b>, Shangchen Zhou, Ruicheng Feng, Jun Jiang, Wenxiu Sun, Qingyu Yang, Qingpeng Zhu, Chen Change Loy, and Jinwei Gu.
                <br>
                <span class="links">
		      <a href="https://mipi-challenge.org/MIPI2022/">Website</a>	
		      
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
<ul>
<li><a href="http://mipi-challenge.org/" target="_blank"><font color="#A52A2A">[Workshop]</font></a> ECCV 2022 Workshop on Mobile Intelligent Photography and Imaging (MIPI)</a></li>
<li><a href="https://attend.ieee.org/mmsp-2022/special-sessions/underwater-multimedia-processing/" target="_blank"><font color="#A52A2A">[Special Session]</font></a> IEEE MMSP 2022 Special Session on Underwater Multimedia Processing</a></li>
<li><a href="https://www.frontiersin.org/research-topics/39049/multimodal-intelligence" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> Frontiers in Signal Processing Special Issue on Multimodal Intelligence</a></li>
<li><a href="https://ieeeoes.org/wp-content/uploads/2021/07/JOE_cfp_AMLM.pdf" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> IEEE Journal of Oceanic Engineering Special Issue on Advanced Machine Learning Methodologies for Underwater Image and Video Processing and Analysis (2021-2022)</a></li>
<li><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/MTAP_SI_CFP.pdf" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> Multimedia Tools and Applications Special Issue on Depth-Related Processing and Applications in Visual Systems (2020-2021)</a></li>
<li><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/SPIC_SI_cfp.pdf" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> Signal Processing: Image Communication Special Issue on Visual Information Processing for Underwater Images and Videos: Theories, Algorithms, and Applications (2019-2020)</a></li>
<li><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/APSIPA-ASC-2019-CfP.pdf" target="_blank"><font color="#A52A2A">[Special Session]</font></a> APSIPA ASC 2019 Special Session on Multi-source Data Processing and Analysis: Models, Methods and Applications (2019-2020)</a></li>

</ul>
<br>
<hr />
-->	

		 


		 
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Miscellaneous:</h2>

 
<ul>
<li><a href="https://unsplash.com/"><font color="#1C86EE">Unsplash</font></a></li>
<li><a href="https://pngtree.com/"><font color="#1C86EE">Pngtree</font></a></li>
<li><a href="https://www.wordclouds.com/"><font color="#1C86EE">WordClouds</font></a></li>
<li><a href="https://emojipedia.org/"><font color="#1C86EE">Emojipedia</font></a></li>
<li><a href="https://film-grab.com/"><font color="#1C86EE">FilmGrab</font></a></li>
<li><a href="https://deviparikh.medium.com/how-we-write-rebuttals-dc84742fece1/"><font color="#1C86EE">How we write rebuttals</font></a></li>
<li><a href="http://www-net.cs.umass.edu/kurose/writing/intro-style.html"><font color="#1C86EE">Writing a good introduction</font></a></li>
<li><a href="https://www.computer.org/publications/tech-news/trends/deep-learning-vs-machine-learning-whats-the-difference?source=cssocial"><font color="#1C86EE">Deep Learning vs Machine Learning: What‚Äôs the Difference</font></a></li>
</ul>
<br>


<!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=pPHWAkKgmzsFC_v7-3ndOuL5q3qL_EhEE16zTJwxtRw"></script> -->
<!--<div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3244445&c=9733648" alt="AmazingCounters.com"></a></div>--> 
<!--div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3230662&c=9692299" alt="AmazingCounters.com"></a></div>-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-156698907-1"></script>-->




</section>

</div>
<!--<script src="javascripts/scale.fix.js"></script>-->
</body>
</html>  
