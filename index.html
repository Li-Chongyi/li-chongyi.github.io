<!--<!doctype html>
<html>-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<head>
<!--<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">-->
  <title>Homepage of Chongyi Li</title>
	<style>

@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
.title { width : 650px; margin : 20px auto; }
.container { width : 700px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 40px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 10px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links { position :relative ; top : 10px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
	</style>
<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<meta name="viewport" content="width=device-width">
<script async="" src="./analytics.js"></script>
</head>
<body>
<div class="wrapper">
<header>
<h7>Chongyi Li</h7><br><br>
<div>
<img src="sub_img/lichongyi_photo.jpg" border="0" width="80%"><br></div><br>

  
<p>
<small>lichongyi25@gmail.com</small><br><br>
<a href="https://github.com/Li-Chongyi/" target="_blank">[GitHub]</a>  
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chongyi" target="_blank">[DBLP]</a>  <br>
<a href="https://scholar.google.com/citations?user=1_I0P-AAAAAJ&hl=zh-CN" target="_blank">[Google Scholar]</a> <br>
</p> <br>
<p class="view"><a href="https://li-chongyi.github.io/">Homepage</a></p>
<p class="view"><a href="sub_publication.html">Publications</a></p>
<p class="view"><a href="sub_projects.html">Projects</a></p>
</header>

<section>

<h2>
<a id="Biography-page" class="anchor" href="#biography-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome to Chongyi Li (李重仪)'s Homepage</h2>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>
<br><strong>2020/01-now</strong>, I join in the MMLab@NTU as a research fellow and work with Dr.<a href="http://personal.ie.cuhk.edu.hk/~ccloy/index.html"><font color="#1C86EE"> Chen Change Loy</font></a>,  School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore.</br>

<br><strong>2018/10-2020/01</strong>, I was a postdoctoral fellow and worked with Prof. <a href="http://www.cs.cityu.edu.hk/~cssamk/research_group/index.html"><font color="#1C86EE">Sam Kwong</font></a> (IEEE Fellow), Department of Computer Science, City University of Hong Kong (CityU), Kowloon, Hong Kong.</br>

<br><strong>2016/12-2017/12</strong>, I was a joint Ph.D. student at Research School of Engineering, Australian National University (ANU), Canberra, Australia, under the supervision of Prof. <a href="http://www.porikli.com/"><font color="#1C86EE">Fatih Porikli</font></a> (IEEE Fellow).</br>

<br><strong>2014/09-2018/07</strong>, I was a Ph.D. student at School of Electrical and Information Engineering, Tianjin University (TJU), Tianjin, China, under the supervision of Prof. <a href="http://seea.tju.edu.cn/szdw/xxx/201709/t20170902_298201.htm"><font color="#1C86EE">Jichang Guo</font></a>.</br>

<hr />
</p>
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Research Interests:</h2>

<ul>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>My primary reseach interests include image processing, computer vision, and deep learning, particularly in the domains of 
  <li><strong>Image and video restoration and enhancement</strong> The purpose is to develop algorithms to process an image or video so that result is more suitable than original image or video for specific application. The specific research topics are</li> 
  <ol type="a" start="1">
      <li>restoring and enhancing the images and videos captured in adverse weather (hazy, foggy, sandy, dusty, rainy, snowy day)</li>
      <li>restoring and enhancing the images and videos captured in special circumstances or devices (underwater, weak illumination, dark, under-display devices)</li>
      <li>general photo enhancement, auto image retouching</li>
      <li>image/depth super-resolution, image debluring, image denosing</li>
  </ol>
  <li><strong>Salient object detection</strong> The purpose is to design AI models to perceivce and understand scenes. The specific research topics are</li>
  <ol type="a" start="1">
      <li>RGB-D salient object detection</li>
      <li>co-salient object detection</li>
      <li>remote sensing image salient object detection</li>
  </ol>  

  
</ul>
<br>
<hr />
</p>
 
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Call for Papers:</h2>  
  
<ul>
<li><font color="#000000"> Special Issue on Depth-Related Processing and Applications in Visual Systems, Multimedia Tools and Applications (SCI, IF: 2.101) <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/MTAP_SI_CFP.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: November 1, 2020. <a href="https://www.springer.com/journal/11042/updates/17918156" target="_blue"><font color="#ff0000">[Offical Link]</font></a></a></li>[Closed]
<li><font color="#000000"> Special Issue on Visual Information Processing for Underwater Images and Videos: Theories, Algorithms, and Applications in Signal Processing : Image Communication (SCI, IF: 2.814) <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/SPIC_SI_cfp.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: July 31, 2020. <a href="https://www.journals.elsevier.com/signal-processing-image-communication/call-for-papers/theories-algorithms-and-applications" target="_blue"><font color="#ff0000">[Offical Link]</font></a></a></li>[Closed]
<li><font color="#000000"> Special Session in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference(APSIPA ASC) 2019.</font><a href="http://www.apsipa2019.org/#" target="_blank"><font color="#ff0000"> [Link]</font>: Special Session on Multi-source Data Processing and Analysis: Models, Methods and Applications <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/APSIPA-ASC-2019-CfP.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a></li>[Closed]

</ul>
<br>
<hr />
  
  
  
  
 
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recent News:</h2>

<ul>
<li> 2021/04 -- One paper has been accepted by <strong>Applied Intelligence (IF: 3.325)</strong></li>
<li> 2021/03 -- One book chapter has been published in <strong>World Scientific</strong></li>
<li> 2021/03 -- One paper has been accepted by <strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (IF: 17.861)</strong></li>
<li> 2021/03 -- One paper hits on <strong>CVPR 2021</strong></li>
<li> 2021/01 -- One paper has been selected as <strong>ESI Highly Cited Paper</strong></li>
<li> 2021/01 -- One paper has been accepted by <strong>Engineering Applications of Artificial Intelligence (IF: 3.526)</strong></li>
<li> 2021/01 -- One paper has been accepted by <strong>Frontiers of Computer Science (IF: 1.940)</strong></li>
<li> 2021/01 -- I am nominated with honor to serve as an <strong>Associate Editor</strong> of the Springer Journal of Signal, Image and Video Processing (IF: 1.794).</strong></li>
<li> 2020/12 -- Two papers have been selected as <strong>ESI Highly Cited Paper</strong></li>
<li> 2020/12 -- One paper has been accepted by <strong>Information Science</strong></li>
<li> 2020/10 -- 2020 National Postdoctoral Forum on the Development and Application of Artificial Intelligence. <strong>Excellence Award</strong></li>
<li> 2020/10 -- One paper has been accepted by <strong>IEEE Trans. Image Processing.</strong> </li>
<li> 2020/09 -- One paper has been accepted by <strong>NeurIPS</strong> </li>
<li>We released a survey about image colorization (Image Colorization: A Survey and Dataset). <a href="https://arxiv.org/pdf/2008.10774.pdf" target="_blank"><font color="#ff0000">[Arxiv Version]</font></a></a></li>
<li><strong>Chongyi Li</strong>, Runmin Cong, Yongri Piao, Qianqian Xu, and Chen Change Loy, <ud2>RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</ud2> is accepted by <strong>ECCV2020</strong>. <a href="https://li-chongyi.github.io/Proj_ECCV20" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li> 
<li>We release the testing code of our Zero-DCE for low-light image enhancement <strong>(CVPR 2020)</strong>. You can find the <a href="https://github.com/Li-Chongyi/Zero-DCE" target="_blank"><font color="#ff0000">[Code]</font></a>. Have fun!</li>
<!--<li><strong>Chongyi Li</strong>, Runmin Cong, Chunle Guo, Hua Li, Chunjie Zhang, Feng Zheng, and Yao Zhao, <ud2>A Parallel Down-Up Fusion Network for Salient Object Detection in Optical Remote Sensing Images</ud2> is accepted by <strong>Neurocomputing</strong>.</li>
<li>We release more details of our Zero-DCE for low-light image enhancement <strong>(CVPR 2020)</strong>. You can find the <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html" target="_blank"><font color="#ff0000">[Project Page]</font></a>. Have fun!</li>
<li><strong>Chongyi Li</strong>, Runmin Cong, Sam Kwong, Junhui Hou, Huazhu Fu, Guopu Zhu, Dingwen Zhang, and Qingming Huang, <ud2>ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D salient Object Detection</ud2> is accepted by <strong>IEEE Transactions on Cybernetics</strong>.</li>
<li>We released the dataset and testing model of our TIP 2019 <ud2>An Underwater Image Enhancement Benchmark Dataset and Beyond</ud2>. Have fun! <a href="https://li-chongyi.github.io/proj_benchmark.html" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li>
<p> If you use this dataseet or code, please cite the related paper. Thanks.</p>
<li><strong>Chongyi Li</strong>, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, Dacheng Tao, <ud2>An Underwater Image Enhancement Benchmark Dataset and Beyond</ud2> is accepted by <strong>IEEE Transactions on Image Processing</strong>. <a href="https://li-chongyi.github.io/proj_benchmark.html" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li> 
<li>We released our PR 2019 Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement (Datasets and Code). Have fun! <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li>
<p> If you use this code, please cite the related paper. Thanks.</p>
-->
</ul>
<br>
<hr />
 
<div class="container">
<h2><a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Projects:</h2>
        <br>
	  <div class="publication">
            <img src="./UDC_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network</a>
                </strong>
		<em><b>CVPR, 2021</b></em>
                <br> 
               Ruicheng Feng, <b>Chongyi Li</b>, Huaijin Chen, Shuai Li, Chen Change Loy, Jinwei Gu.
                <br>
                <span class="links">
                    <a href="">PDF</a>| 
                    <a href="">Project Page</a>| 
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />
	<br />	
	
	  <div class="publication">
            <img src="./Zero-DCE_files/zerodce++.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/9369102">Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation</a>
                </strong>
		<em><b>TPAMI, 2021</b></em>
                <br>
               <b>Chongyi Li</b>, Chunle Guo, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9369102">PDF</a>| 
                    <a href="https://li-chongyi.github.io/Proj_Zero-DCE++.html">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/Zero-DCE_extension">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	
        <div class="publication">
            <img src="./Zero-DCE_files/framework.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf">Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement</a>
                </strong>
		<em><b>CVPR, 2020</b></em>
                <br>
                Chunle Guo*, <b>Chongyi Li*</b>,  Jichang Guo, Chen Change Loy,  Junhui Hou,  Sam Kwong,  Runmin Cong.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf">PDF</a>| 
                    <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/Zero-DCE">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	

        <div class="publication">
            <img src="./ECCV20_files/ECCV20-visual_results.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</a>
                </strong>
		<em><b>ECCV, 2020</b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Yongri Piao, Qianqian Xu, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222.pdf">PDF</a>| 
                    <a href="https://li-chongyi.github.io/Proj_ECCV20">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/cmMS-ECCV20">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
        <div class="publication">
            <img src="./ACMMM_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://dl.acm.org/doi/10.1145/3394171.3413928">NuI-Go: Recursive Non-Local Encoder-Decoder Network for Retinal Image Non-Uniform Illumination Removal</a>
                </strong>
		<em><b>ACM MM, 2020</b></em>
                <br>
                <b>Chongyi Li</b>, Huazhu Fu, Runmin Cong, Zechao Li, Qianqian Xu.
                <br>
                <span class="links">
                    <a href="https://dl.acm.org/doi/10.1145/3394171.3413928">PDF</a>| 
                    <a href="Proj_ACMMM20_NuI-Go.html">Project Page</a>| 
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
        <div class="publication">
            <img src="./benchmark_logo.png" class="publogo" height="110" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8917818">An Underwater Image Enhancement Benchmark Dataset and Beyond</a>
                </strong>
		 <em><b>TIP, 2020</b></em>
                <br>
                <b>Chongyi Li</b>, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, Dacheng Tao.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8917818">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_benchmark.html">Project Page</a>| 
		    <a href="https://drive.google.com/file/d/12W_kkblc2Vryb9zHQ6BfGQ_NKUfXYk13/view">Benchmark</a>| 
                    <a href="https://github.com/Li-Chongyi/Water-Net_Code">Code</a>
                </span>
           </p>
        </div>
        <br>
        <br>
	<br />
	<br />
        <div class="publication">
            <img src="./tgars_logo.png" class="publogo" height="130" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8793227">Nested Network With Two-Stream Pyramid for Salient Object Detection in Optical Remote Sensing Images</a>
                </strong>
		 <em><b>TGARS, 2020</b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Junhui Hou, Sanyi Zhang, Yue Qian, Sam Kwong.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8793227">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_optical_saliency.html">Project Page</a>| 
		    <a href="https://drive.google.com/file/d/15FOnRo1xnz05fcNkXBhWy8WL0C26i8y4/view">Benchmark</a>| 
                    <a href="https://drive.google.com/file/d/1nnZKphu9_4oBvie4yqdxG95tYZRsqj4W/view">Results</a>
                </span>
           </p>
        </div>
        <br>
        <br>
	<br />
	<br />
        <div class="publication">
            <img src="./PR_logo.jpg" class="publogo" height="130" width="300 px">
            <p> 
                <strong>
                    <a href="https://www.sciencedirect.com.remotexs.ntu.edu.sg/science/article/pii/S0031320319303401">Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement</a>
                </strong>
		 <em><b>PR, 2020</b></em>
                <br>
                <b>Chongyi Li</b>, Saeed Anwar, Fatih Porikli.
                <br>
                <span class="links">
                    <a href="https://www.sciencedirect.com.remotexs.ntu.edu.sg/science/article/pii/S0031320319303401">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html">Project Page</a>| 
                    <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html">Code</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
        <div class="publication">
            <img src="./TC2020_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/stamp/stamp.jsp?tp=&arnumber=8998588">ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D Salient Object Detection</a>
                </strong>
		 <em><b>TCyb, 2020</b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Sam Kwong, et al.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/stamp/stamp.jsp?tp=&arnumber=8998588">PDF</a>| 
                    <a href="https://github.com/Li-Chongyi/ASIF-Net">Code and Results</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
        <div class="publication">
            <img src="./NeurIPS2020_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://proceedings.neurips.cc/paper/2020/file/4dc3ed26a29c9c3df3ec373524377a5b-Paper.pdf">CoADNet: Collaborative Aggregation-and-Distribution Networks for Co-Salient Object Detection</a>
                </strong>
		 <em><b>NeurIPS, 2020</b></em>
                <br>
                Qijian Zhang, Runmin Cong, Junhui Hou, <b>Chongyi Li</b>, and Yao Zhao.
                <br>
                <span class="links">
                    <a href="https://proceedings.neurips.cc/paper/2020/file/4dc3ed26a29c9c3df3ec373524377a5b-Paper.pdf">PDF</a>| 
                    <a href="https://rmcong.github.io/proj_CoADNet.html">Project Page</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
        <div class="publication">
            <img src="./TIP_RS2020_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292434">Dense attention fluid network for salient object detection in optical remote sensing images</a>
                </strong>
		 <em><b>TIP, 2020</b></em>
                <br>
                Qijian Zhang, Runmin Cong, <b>Chongyi Li</b>, Ming-Ming Cheng, Yuming Fang, Xiaochun Cao, Yao Zhao, and Sam Kwong.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292434">PDF</a>| 
                    <a href="https://rmcong.github.io/proj_DAFNet.html">Project Page</a>
                    <a href="https://github.com/rmcong/EORSSD-dataset">Dataset</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
        <div class="publication">
            <img src="./depth_SR_logo.png" class="publogo" height="130" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8579111">Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution</a>
                </strong>
		<em><b>TIP, 2019</b></em>
                <br>
                Chunle Guo*, <b>Chongyi Li*</b>, Jichang Guo, Runmin Cong, Huazhu Fu, Ping Han.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8579111">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_SR.html">Project Page</a>| 
                    <a href="https://drive.google.com/file/d/18y6jpGnjqYINzMFJeVHcg4SQiYM-lFBb/view">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>

</div>
<br>
<hr />



<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Participating Fundings:</h2>

<ul>
<li>"Study on the hierarchical modeling of underwater imaging and underwater image/video clearness method", The National Natural Science Foundation of China (NSFC). (Principal Participator) 
</ul>
<br>
<hr />
  
  
  
 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Honors & Awards:</h2>

<ul>
<li>Three papers have been selected as ESI Highly Cited Paper.</li>
<li>2020 National Postdoctoral Forum on the Development and Application of Artificial Intelligence, Tianjin, China. Excellence Award.</li>
<li>Distinguished Dissertation Award of Beijing Society of Image and Graphics, 2018.</li>
<li>"Outstanding Graduate Student" in Tianjin University, 2018.</li>
<!--<li>Bohai Securities Fellowship, 2017.</li>
<li>"Merit Student" in Tianjin University, 2017.</li>
<li>"Advanced Individual" in the creative working, 2017.</li>
<li>China Scholarship Council (CSC) scholarships, 2016.</li>
<li>The First Class Academic Scholarship in Tianjin University, 2015, 2016.</li>
<li>"Advanced Individual" in international exchange, 2015.</li>
<li>"Advanced Individual" in the creative working, 2015.</li>-->

</ul>
<br>
<hr /> 

 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Teaching:</h2>
<ul>
<li>NTU AI6126: Advanced Computer Vision, Teaching Assistant, 2020.</li>
</ul>
<br>
<hr /> 

 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Professional Service:</h2>

<ul>
<br><strong>Associate Editor</strong></br>
Springer Journal of Signal, Image and Video Processing (IF: 1.794)

<br><strong>Guest Editor</strong></br>
Signal Processing : Image Communication

<br>Multimedia Tools and Applications

<br><strong>Reviewer</strong></br>
<li><strong>IEEE</strong>:</li>TPAMI, TIP, TCyber, TCSVT, TNNLS, TMM, TB, TIM, TGARS, TITS, TCSVT II: Express Briefs, ACCESS, SPL, JOE, JBHI, J-STARS, MultiMedia

<li><strong>Elsevier</strong>:</li> CVIU, JOE, DSP, Neurocomputing, PRL; SPIC, JCG; Knowledge-based Systems 

<li><strong>Springer</strong>:</li> IJCV, VC, SCIS,  MVA,  EURASIP JIVP

<li><strong>ACM</strong>:</li> Computing Surveys

<li><strong>IET</strong>:</li> CV

<li><strong>SPIE</strong>:</li> JEI

<li><strong>Conference</strong>:</li>ICPR 2018; ACM MM 2019;ICME 2020; ACM MM2020; MICCAI 2020; AAAI 2021; CVPR 2021; IJCAI 2021;ICME 2021; MICCAI 2021; ICCV2021
</ul>
<br>
<hr />    
  
<!--   
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cooperators:</h2>


<ul>
<li><a href="http://www.cs.cityu.edu.hk/~cssamk/research_group/index.html"><font color="#1C86EE">Sam Kwong (IEEE Fellow and Professor at CityU, Hong Kong, China)</font></a></li>
<li><a href="http://www.porikli.com/"><font color="#1C86EE">Fatih Porikli (IEEE Fellow and Professor at ANU, Australia)</font></a></li>
<li><a href="https://sydney.edu.au/engineering/people/dacheng.tao.php"><font color="#1C86EE">Dacheng Tao (IEEE Fellow and Professor at TUS, Australia)</font></a></li>
<li><a href="https://sites.google.com/site/junhuihoushomepage/home"><font color="#1C86EE">Junhui Hou (Assistant Professor at CityU, Hong Kong, China)</font></a></li>
<li><a href="https://sites.google.com/site/renwenqi888/"><font color="#1C86EE">Wenqi Ren (Assistant Professor at State Key Laboratory of Information Security, IIE, CAS)</font></a></li>
<li><a href="https://scholar.google.com/citations?user=vPJIHywAAAAJ&hl=zh-CN&oi=ao"><font color="#1C86EE">Saeed Anwar (Research Fellow at ANU, CSIRO/Data61, Australia)</font></a></li>
<li><a href="https://rmcong.github.io/"><font color="#1C86EE">Runmin Cong (Associate Professor, at Institute of Information Science, Beijing Jiaotong University， China)</font></li> 
</ul>
<br>
 --> 
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=pPHWAkKgmzsFC_v7-3ndOuL5q3qL_EhEE16zTJwxtRw"></script>
<!--<div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3244445&c=9733648" alt="AmazingCounters.com"></a></div>--> 
<!--div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3230662&c=9692299" alt="AmazingCounters.com"></a></div>-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-156698907-1"></script>




</section>

</div>
<!--<script src="javascripts/scale.fix.js"></script>-->
</body>
</html>  
