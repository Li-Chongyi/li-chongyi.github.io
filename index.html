<!--<!doctype html>
<html>-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<head>
 <link rel="Shortcut Icon" href="./logo/hp_logo.jpg" sizes=16x16  type="image/x-icon" />
 <link rel="Bookmark" href="./logo/hp_logo.jpg" sizes=16x16 type="image/x-icon" />
<!--<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">-->
  <title>Chongyi Li</title>
	<style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
.title { width : 650px; margin : 20px auto; }
.container { width : 700px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 40px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 10px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 0px; font-weight: normal;}
.publication strong a { color : #0000A0; }
.publication .links { position :relative ; top : 10px }
.publication .links a { margin-right : 5px; font-size: 15px; font-weight: normal}
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
li, ul {font-weight: normal;}
</style>
<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<meta name="viewport" content="width=device-width">
<script async="" src="./analytics.js"></script>
</head>
<body>
<div class="wrapper">
<header>
<h7>Chongyi Li</h7><br><br>
<div>
<img src="sub_img/lichongyi_photo_3.png" border="0" width="90%"><br></div><br>

  
<p>
<small>üìç ABN-02b-11, NTU, SG</small><br>
<small>üìßlichongyi25 at gmail.com</small><br>
<small>üìßchongyi.li at ntu.edu.sg</small><br><br>
<a href="https://github.com/Li-Chongyi/" target="_blank">[GitHub]</a>  
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chongyi" target="_blank">[DBLP]</a>  <br>
<a href="https://scholar.google.com/citations?user=1_I0P-AAAAAJ&hl=en" target="_blank">[Google Scholar]</a> <br>
</p> <br>
	
	
<p class="view"><a href="https://li-chongyi.github.io/">Homepage</a></p>
<p class="view"><a href="sub_publication.html">Publications</a></p>
<p class="view"><a href="datasets.html">Datasets</a></p>

	
<!--<p class="view"><a href="sub_projects.html">Projects</a></p>-->
</header>

<section>

<h2>
<a id="Biography-page" class="anchor" href="#biography-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome to Chongyi Li (ÊùéÈáç‰ª™)'s Homepage</h2>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>
<br><strong>2021/10 - </strong>, I am a Research Assistant Professor with the School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore.</br>
<br><strong>2020/01 - 2021/10</strong>, I was a research fellow at the <a href="https://www.mmlab-ntu.com/"><font color="#1C86EE"> MMLab@NTU</font></a> and worked with Dr.<a href="http://personal.ie.cuhk.edu.hk/~ccloy/index.html"><font color="#1C86EE"> Chen Change Loy</font></a>,  School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore.</br>
<br><strong>2018/10 - 2020/01</strong>, I was a postdoctoral fellow and worked with Chair Prof. <a href="http://www.cs.cityu.edu.hk/~cssamk/research_group/index.html"><font color="#1C86EE">Sam Kwong</font></a> (IEEE Fellow), Department of Computer Science, City University of Hong Kong (CityU), Kowloon, Hong Kong.</br>
<br><strong>2016/12 - 2017/12</strong>, I was a joint Ph.D. student at Research School of Engineering, Australian National University (ANU), Canberra, Australia, under the supervision of Prof. <a href="http://www.porikli.com/"><font color="#1C86EE">Fatih Porikli</font></a> (IEEE Fellow).</br>
<br><strong>2014/09 - 2018/07</strong>, I was a Ph.D. student at School of Electrical and Information Engineering, Tianjin University (TJU), Tianjin, China, under the supervision of Prof. <a href="http://seea.tju.edu.cn/info/1014/1453.htm"><font color="#1C86EE">Jichang Guo</font></a>.</br>

<hr />
</p>
 
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hiring:</h2>
<br><p><font color="blue">We are looking for Research Fellow, Research Assistant, and Project Officer who want to conduct
research and develop advanced deep learning algorithms for image and video enhancement and restoration, computational imaging, and image signal processor. <a href="https://www.mmlab-ntu.com/careers.html" target="_blank"><font color="#ff0000">[Join Us]</font></a></font></p></br>
</ul>
<br>	

<!--
<hr />
</p>
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Research Interests:</h2>

<ul>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>My primary reseach interests include artificial intelligence, machine learning, computer vision, and image processing, particularly in the domains of 
  <li><strong>Image and Video Restoration and Enhancement</strong></li> <br/>
	The purpose is to develop algorithms to process an image or video so that result is more suitable than original image or video for specific application. The specific research topics are 
  <ol type="a" start="1">
      <li>restoring and enhancing the images and videos captured in adverse weather (hazy, foggy, sandy, dusty, rainy, snowy day)</li>
      <li>restoring and enhancing the images and videos captured in special circumstances or devices (underwater, weak illumination, dark, under-display devices)</li>
      <li>general photo enhancement, auto image retouching</li>
      <li>image/depth super-resolution, image deblurring, image denosing</li>
  </ol>
  <li><strong>Multi-Modality Scene Understanding</strong> </li><br/>
	  The purpose is to design AI models to perceive and understand scenes. The specific research topics are
  <ol type="a" start="1">
      <li>RGB-D salient object detection</li>
      <li>co-salient object detection</li>
      <li>remote sensing image salient object detection</li>
  </ol>  
-->
  
</ul>
<br>
<hr />
</p>
 
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recent News:</h2>
 
<ul>
<li> 2022/12 --One paper has been recognized as <strong>ESI Highly Cited Paper</strong></li>
<li> 2022/11 --One paper got accepted by <strong>AAAI 2023</strong>. The code is released. </li>
<li> 2022/11 --Excited to announce two papers have been accepted by NeurIPS 2022 as <strong>Spotlight</strong>!</li>
<li> 2022/10 --I am recognized as <strong>Outstanding Reviewer</strong>, ECCV 2022</li>
<li> 2022/10 --I am homored to be recognised as the <strong>World‚Äôs Top 2% Scientists (2022)</strong>. It is compiled by Stanford University based on the standardized citation indicators, which is avaiable online at <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/4" target="_blank"><font color="#ff0000">Mendeley Database</font></a></li>
<li> 2022/10 --One paper has been accepted by <strong>TIP</strong></li>
<li> 2022/10 --We release our LEDNet code and dataset (ECCV 2022) </li>
<li> 2022/09 --Three papers hit on <strong>NeurIPS 2022</strong>+one paper hits on <strong>NeurIPS 2022-Datasets and Benchmarks track</strong></li>
<li> 2022/08 --ZeroDCE++ is included in the list of <strong>Popular TPAMI Articles</strong></li>
<li> 2022/07 --One paper has been accepted by <strong>TIP</strong></li>
<li> 2022/07 --Our paper Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement has been recognized as <strong>2020 Pattern Recognition Best Paper Honourable Mention.</strong></li>
<li> 2022/07 --I am nominated with honor to serve as an <strong>Area Chair</strong> of the BMVC 2022.</strong></li>
<li> 2022/07 --We release the code of our CodeFormer (face restoration). Try your face on <a href="https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing" target="_blank"><font color="#ff0000">[Colab].</font></a></li>
<li> 2022/07 --I am nominated with honor to serve as a <strong>Senior Program Committee</strong> of the AAAI 2023.</strong></li>
<li> 2022/07 --One paper hits on <strong>ECCV 2022</strong></li>
<li> 2022/07 --We release our Dehamer code (CVPR 2022) </li>
<li> 2022/06 --Two papers hit on <strong>ACMM MM 2022</strong></li>
<li> 2022/06 --One paper has been accepted by <strong>TIP</strong></li>
<li> 2022/05 --One paper has been recognized as <strong>ESI Hot Paper</strong></li>
<li> 2022/05 --One paper has been accepted by <strong>TIP</strong></li>
<li> 2022/05 --I am nominated with honor to serve as an <strong>Associate Editor</strong> of IEEE Journal of Oceanic Engineering (IF: 3.554).</strong></li>
<li> 2022/04 --Our team Feedforward won the <strong>second runner-up (3/317)</strong> in <a href="https://studio.brainpp.com/competition/5?name=2022%20MegCup%20%E7%82%BC%E4%B8%B9%E5%A4%A7%E8%B5%9B&tab=rank" target="_blank"><font color="#ff0000">MegCup Efficient Model for Blind Denoising</font></a>. Congrats to Xin Jin, Ruiqi Wu, and Zhen Li. The code is publicly available <a href="https://github.com/Srameo/megcup-feedforward" target="_blank"><font color="#ff0000">here.</font></a></li>
<li> 2022/03 --Our ECCV 2022 Workshop coupled with challenges: <strong>Mobile Intelligent Photography & Imaging</strong> has been accepted. <a href="http://mipi-challenge.org/" target="_blank"><font color="#ff0000">(http://mipi-challenge.org/)</font></a></li>
<li> 2022/03 --One paper has been accepted by <strong>CVPR 2022</strong></li>
<li> 2022/02 --I am homored to be recognised as the <strong>Outstanding Reviewer</strong> of the IEEE Journal of Oceanic Engineering.</li>
<li> 2022/01 --One paper has been recognized as <strong>ESI Hot Paper</strong></li>
<li> 2022/01 --One paper has been recognized as <strong>ESI Highly Cited Paper</strong></li>
<li> 2022/01 --One papers is recognized as  <strong>Popular Documents</strong> <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=6221036" target="_blank"><font color="#ff0000">(the 50 most frequently accessed documents)</font></a> in  <strong>IEEE Transactions on Cybernetics</strong></li>
<li> 2022/01 --I am nominated with honor to serve as an <strong>Associate Editor</strong> of Neurocomputing (IF: 5.719).</strong></li>
<li> 2022/01 --Two papers are recognized as  <strong>Popular Documents</strong> <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=83" target="_blank"><font color="#ff0000">(the 50 most frequently accessed documents)</font></a> in  <strong>IEEE Transactions on Image Processing</strong></li>
<li> 2022/01 --I am homored to be recognised as the <strong>World‚Äôs Top 2% Scientists (2020)</strong>. It is compiled by Stanford University based on the standardized citation indicators (Table_1_Authors_singleyr_2020_wopp_extracted_202108), which is avaiable online at <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/3" target="_blank"><font color="#ff0000">Mendeley Database</font></a></li>
<!--
<li> 2021/12 --One paper has been accepted by <strong>IEEE Transactions on Geoscience and Remote Sensing  (IF: 5.6)</strong></li>
<li> 2021/11 --One paper has been accepted by <strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (IF: 17.861)</strong></li>
<li> 2021/11 --I received the runners-up award of  <strong>Innovation in Information Technology Application and Artificial Intelligent Academic Forum for Post-doctoral Talents, Tianjin, China, 2021</strong>. </li>
<li> 2021/10 --One paper has been accepted by <strong>International Conference on 3D Vision 2021</strong> </li>
<li> 2021/09 --My supervised FYP student is awarded the <strong>2021 Global Undergraduate Awards</strong> for his entry 'Learning to See in the Dark - Low Light Image Enhancement'.  <a href="https://undergraduateawards.com/" target="_blank"><font color="#ff0000">[The Global Undergraduate Awards]</font></a> Congratulations to Sihao! So proud of you! </li>
<li> 2021/09 --I am recognized as <strong>Outstanding Reviewer</strong>, IEEE ICCV 2021</li>
<li> 2021/08 --I am nominated with honor to serve as a <strong>Senior Program Committee (Meta-Reviewers)</strong> of the AAAI 2022.</strong></li>
<li> 2021/08 --We organize a <strong>special issue</strong> on Advanced Machine Learning Methodologies for Underwater Image and Video Processing and Analysis in IEEE Journal of Oceanic Engineering (IEEE-JOE) (SCI, IF: 3.554)</li>
<li> 2021/07 --Our Zero-DCE (Zero-DCE++) for low-light image enhancement was used in the 1st place solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition <a href="https://arxiv.org/abs/2107.00818" target="_blank"><font color="#ff0000">[1st place solution]</font></a>. Check our Zero-DCE (Zero-DCE++) <a href="https://li-chongyi.github.io/Proj_Zero-DCE++.html" target="_blank"><font color="#ff0000">[Project Page]</font></a>.</li>
<li> 2021/05 --I am recognized as <strong>Outstanding Reviewer</strong>, IEEE CVPR 2021</li>
<li> 2021/05 -- One paper has been recognized as <strong>ESI Highly Cited Paper</strong></li>
<li> 2021/04 -- We release the code of our Zero-DCE++ (TPAMI2021).</li>
<li> 2021/04 -- We release the code of our Ucolor (TIP2021).</li>
<li> 2021/04 -- Invited talk@<strong>SenseTime</strong> about AI Photo Enhancement. You can find the video @<strong>bilibili</strong> <a href="https://www.bilibili.com/video/BV1Gv411j7HN?p=2" target="_blank"><font color="#ff0000">[Video in Chinese]</font></a>. </li>
<li> 2021/04 -- We release our deep learning-based low-light image enhancement platform <strong>LoLi-Platform</strong>. You can find the <a href="https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open/" target="_blank"><font color="#ff0000">[LoLi-Platform]</font></a>. <a href="https://arxiv.org/abs/2104.10729" target="_blank"><font color="#ff0000">[Survey]</font></a>. Have fun!</li>
<li> 2021/04 -- We release the code of our Under-Display Camera Image Restoration <strong>(CVPR 2021)</strong>. You can find the <a href="https://jnjaby.github.io/projects/UDC/" target="_blank"><font color="#ff0000">[Code]</font></a>. Have fun!</li>
<li> 2021/04 -- One paper has been accepted by <strong>IEEE Transactions on Image Processing</strong> </li>
<li> 2021/04 -- 6th, NTIRE 2021 Challenge on Non-Homogeneous Image Dehazing</li>
<li> 2021/04 -- One paper has been accepted by <strong>IEEE Geoscience and Remote Sensing Letters  (IF: 3.833)</strong></li>
<li> 2021/04 -- One paper has been accepted by <strong>Applied Intelligence (IF: 3.325)</strong></li>
<li> 2021/03 -- One book chapter has been published in <strong>World Scientific</strong></li>
<li> 2021/03 -- One paper has been accepted by <strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (IF: 17.861)</strong></li>
<li> 2021/03 -- One paper hits on <strong>CVPR 2021</strong></li>
<li> 2021/01 -- One paper has been recognized as <strong>ESI Highly Cited Paper</strong></li>
<li> 2021/01 -- One paper has been accepted by <strong>Engineering Applications of Artificial Intelligence (IF: 3.526)</strong></li>
<li> 2021/01 -- One paper has been accepted by <strong>Frontiers of Computer Science (IF: 1.940)</strong></li>
<li> 2021/01 -- I am nominated with honor to serve as an <strong>Associate Editor</strong> of the Springer Journal of Signal, Image and Video Processing (IF: 1.794).</strong></li>
<li> 2020/12 -- Two papers have been recognized as <strong>ESI Highly Cited Paper</strong></li>
<li> 2020/12 -- One paper has been accepted by <strong>Information Science</strong></li>
<li> 2020/10 -- 2020 National Postdoctoral Forum on the Development and Application of Artificial Intelligence. <strong>Excellence Award</strong></li>
<li> 2020/10 -- One paper has been accepted by <strong>IEEE Transactions on Image Processing</strong> </li>
<li> 2020/09 -- One paper has been accepted by <strong>NeurIPS</strong> </li>
<li>We released a survey about image colorization (Image Colorization: A Survey and Dataset). <a href="https://arxiv.org/pdf/2008.10774.pdf" target="_blank"><font color="#ff0000">[Arxiv Version]</font></a></a></li>
<li><strong>Chongyi Li</strong>, Runmin Cong, Yongri Piao, Qianqian Xu, and Chen Change Loy, <ud2>RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</ud2> is accepted by <strong>ECCV2020</strong>. <a href="https://li-chongyi.github.io/Proj_ECCV20" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li> 
<li>We release the testing code of our Zero-DCE for low-light image enhancement <strong>(CVPR 2020)</strong>. You can find the <a href="https://github.com/Li-Chongyi/Zero-DCE" target="_blank"><font color="#ff0000">[Code]</font></a>. Have fun!</li>
<li><strong>Chongyi Li</strong>, Runmin Cong, Chunle Guo, Hua Li, Chunjie Zhang, Feng Zheng, and Yao Zhao, <ud2>A Parallel Down-Up Fusion Network for Salient Object Detection in Optical Remote Sensing Images</ud2> is accepted by <strong>Neurocomputing</strong>.</li>
<li>We release more details of our Zero-DCE for low-light image enhancement <strong>(CVPR 2020)</strong>. You can find the <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html" target="_blank"><font color="#ff0000">[Project Page]</font></a>. Have fun!</li>
<li><strong>Chongyi Li</strong>, Runmin Cong, Sam Kwong, Junhui Hou, Huazhu Fu, Guopu Zhu, Dingwen Zhang, and Qingming Huang, <ud2>ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D salient Object Detection</ud2> is accepted by <strong>IEEE Transactions on Cybernetics</strong>.</li>
<li>We released the dataset and testing model of our TIP 2019 <ud2>An Underwater Image Enhancement Benchmark Dataset and Beyond</ud2>. Have fun! <a href="https://li-chongyi.github.io/proj_benchmark.html" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li>
<p> If you use this dataseet or code, please cite the related paper. Thanks.</p>
<li><strong>Chongyi Li</strong>, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, Dacheng Tao, <ud2>An Underwater Image Enhancement Benchmark Dataset and Beyond</ud2> is accepted by <strong>IEEE Transactions on Image Processing</strong>. <a href="https://li-chongyi.github.io/proj_benchmark.html" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li> 
<li>We released our PR 2019 Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement (Datasets and Code). Have fun! <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li>
<p> If you use this code, please cite the related paper. Thanks.</p>
-->
</ul>
<br>
<hr />

<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Low-Light Image Enhancement Platform: LLIE-Platform</h2>  
  
<ul>
<li> Different deep models may be implemented in different platforms such as Caffe, Theano, TensorFlow, and PyTorch. As a result, different algorithms demand different configurations, GPU versions, and hardware specifications. Such requirements are prohibitive to many researchers, especially for beginners who are new to this area and may not even have GPU resources.</font></a></li>
<li> To resolve these problems, we develop an online platform, LLIE-Platform <a href="http://mc.nankai.edu.cn/ll" target="_blank"><font color="#ff0000">[LLIE-Platform]</font></a>. 
If you use this platform, please cite our paper "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", TPAMI, 2021.</li>
</ul>
<br>
<hr />
-->


<div class="container">
<h2><a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Selected Publications:</h2>
        <br>
	
	  <div class="publication">
            <img src="logo/Cudi.jpg" onmouseover="this.src='logo/Cudi.jpg';" onmouseout="this.src='logo/Cudi.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment</a>
                </strong>
		  <br> 
		<em><b>2022</b></em>
                <br> 
              <b>Chongyi Li</b>, Chunle Guo, Ruicheng Feng, Shangchen Zhou, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2207.14273.pdf">PDF</a>| 
		    <a href="https://li-chongyi.github.io/CuDi_files/">Project Page</a>| 	
                    <a href="https://li-chongyi.github.io/CuDi_files/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />

	<div class="publication">
            <img src="logo/URanker.png" onmouseover="this.src='logo/URanker.png';" onmouseout="this.src='logo/URanker.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Underwater Ranker: Learn Which Is Better and How to Be Better</a>
                </strong>
		  <br> 
		<em><b>AAAI, 2023</b></em>
                <br> 
              Chunle Guo, Ruiqi Wu, Xin Jin, Linghao Han, Zhi Chai, Weidong Zhang, and <b>Chongyi Li<sup>+</sup></b>.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2208.06857.pdf">PDF</a>| 
		    <a href="https://github.com/RQ-Wu/UnderwaterRanker">Project Page</a>| 	
                    <a href="https://github.com/RQ-Wu/UnderwaterRanker">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
<!--	
	<div class="publication">
 <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/BeautyREC.mp4"> <source src="logo/BeautyREC.mp4" type="video/mp4"></video>                
<img src="logo/BeautyREC.jpg" onmouseover="this.src='logo/BeautyREC.jpg';" onmouseout="this.src='logo/BeautyREC.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">BeautyREC: Robust, Efficient, and Content-preserving Makeup Transfer</a>
                </strong>
		  <br> 
		<em><b>2022</b></em>
                <br> 
              Qixin Yan, Chunle Guo, Jixin Zhao, Yuekun Dai, Chen Change Loy, and <b>Chongyi Li<sup>+</sup></b>.
                <br>
                <span class="links">
                    <a href="">PDF</a>| 
		    <a href="">Project Page</a>| 	
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
-->
	<!--
   <div class="publication">
            <img src="logo/R2D2_input.jpg" onmouseover="this.src='logo/R2D2_ours.jpg';" onmouseout="this.src='logo/R2D2_input.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Revisiting RAW Data for Seeing in the Dark</a>
                </strong>
		  <br> 
		<em><b>2022</b></em>
                <br> 
              Linghao Han, Xin Jin, Zhen Li, Chunle Guo, Zhi Chai, and <b>Chongyi Li</b>.
                <br>
                <span class="links">
                    <a href="">PDF</a>| 
		    <a href="">Project Page</a>| 	
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
-->		  <div class="publication">
            <img src="logo/Continuous_Fourier_transform_of_rect_and_sinc_functions.gif" onmouseover="this.src='logo/Continuous_Fourier_transform_of_rect_and_sinc_functions.gif';" onmouseout="this.src='logo/Continuous_Fourier_transform_of_rect_and_sinc_functions.gif';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Deep Fourier Up-Sampling</a>
                </strong>
		  <br> 
		<em><b>NeurIPS, 2022 <a href="" target="_blank"><font color="#ff0000">[Spotlight]</font></a></b></em>
                <br> 
              Man Zhou, Hu Yu, Jie Huang, Feng Zhao, Jinwei Gu, Chen Change Loy, Deyu Meng, and <b>Chongyi Li<sup>+</sup></b>.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2210.05171">PDF</a>| 
		    <a href="https://li-chongyi.github.io/FourierUp_files/">Project Page</a>| 	
                    <a href="https://github.com/manman1995/Deep-Fourier-Upsampling">Code</a>
                </span>
            </p>
          </div>
          <br>
	<br />
	
	  <div class="publication">
            <!--<img src="logo/codeformer1.jpg" onmouseover="this.src='logo/codeformer2.jpg';" onmouseout="this.src='logo/codeformer1.jpg';" class="publogo"  width="300 px">-->
         <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/film-enhancement-codeformer.mp4"> <source src="logo/film-enhancement-codeformer.mp4" type="video/mp4"></video>    
	<p> 
                <strong>
                    <a href="">Towards Robust Blind Face Restoration with Codebook Lookup Transformer</a>
                </strong>
		  <br> 
		<em><b>NeurIPS, 2022</b></em>
                <br> 
                Shangchen Zhou, Kelvin C. K. Chan, <b>Chongyi Li</b>, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2206.11253.pdf">PDF</a>| 
		    <a href="https://shangchenzhou.com/projects/CodeFormer/">Project Page</a>| 	
		    <a href="https://github.com/sczhou/CodeFormer">Code</a>| 
		    <a href="https://www.youtube.com/watch?v=d3VDpkXlueI">Video</a>| 
		    <a href="https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing">Colab</a>
                </span>
            </p>
          </div>
          <br>
	<br>
       <br />
	<br />
	
		  <div class="publication">
            <img src="logo/filter.png" onmouseover="this.src='logo/filter.png';" onmouseout="this.src='logo/filter.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Panchromatic and Multispectral Image Fusion via Alternating Reverse Filtering Network</a>
                </strong>
		  <br> 
		<em><b>NeurIPS, 2022 <a href="" target="_blank"><font color="#ff0000">[Spotlight]</font></a></b></em>
                <br> 
              Keyu Yan, Man Zhou, Jie Huang, Chengjun Xie, Feng Zhao, <b>Chongyi Li</b>, Danfeng Hong.
                <br>
                <span class="links">
                    <a href="">PDF</a>| 
		    <a href="">Project Page</a>| 	
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	  <div class="publication">
            <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/7K_flare.mp4"> <source src="logo/7K_flare.mp4" type="video/mp4"></video>    
            <p> 
                <strong>
                    <a href="">Flare7K: A Phenomenological Nighttime Flare Removal Dataset</a>
                </strong>
		  <br> 
		<em><b>NeurIPS, 2022</b></em>
                <br> 
               Yuekun Dai, <b>Chongyi Li</b>, Shangchen Zhou, Ruicheng Feng, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2210.06570">PDF</a>| 
		    <a href="https://nukaliad.github.io/projects/Flare7K">Project Page</a>| 	
		    <a href="https://drive.google.com/file/d/1PPXWxn7gYvqwHX301SuWmjI7IUUtqxab/view">Dataset</a>|
		  <a href="https://www.youtube.com/watch?v=CR3VFj4NOQM&feature=youtu.be">Video</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br>
	<br />
	<br />
	<br />
	

	


	 <div class="publication">
            <!--<img src="logo/LEDNet_real_input1.jpg" onmouseover="this.src='logo/LEDNet_real_output1.jpg';" onmouseout="this.src='logo/LEDNet_real_input1.jpg';" class="publogo"  width="300 px">-->
        <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/LEDNet_video_demo.mp4"> <source src="logo/LEDNet_video_demo.mp4" type="video/mp4"></video>        
	<p> 
                <strong>
                    <a href="">LEDNet: Joint Low-light Enhancement and Deblurring in the Dark</a>
                </strong>
		  <br> 
		<em><b>ECCV, 2022</b></em>
                <br> 
               Shangchen Zhou, <b>Chongyi Li</b>, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2202.03373.pdf">PDF</a>| 
		    <a href="https://shangchenzhou.com/projects/LEDNet/">Project Page</a>| 	
                    <a href="https://github.com/sczhou/LEDNet/">Code</a>|
		    <a href="https://drive.google.com/drive/folders/11HcsiHNvM7JUlbuHIniREdQ2peDUhtwX//">Dataset</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	<div class="publication">
             <a href="https://imgsli.com/MTI4ODE4"><img src="logo/DeHamer.jpg" onmouseover="this.src='logo/DeHamer.jpg';" onmouseout="this.src='logo/DeHamer.jpg';" class="publogo"  width="300 px"></a>
            <p> 
                <strong>
                    <a href="">Image Dehazing Transformer with Transmission-Aware 3D Position Embedding</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2022</b></em>
                <br> 
              Chunle Guo, Qixin Yan, Saeed Anwar, Runmin Cong, Wenqi Ren, and <b>Chongyi Li<sup>+</sup></b>.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.pdf">PDF</a>| 
		    <a href="https://li-chongyi.github.io/Proj_DeHamer.html">Project Page</a>| 	
                    <a href="https://github.com/Li-Chongyi/Dehamer">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br /> 
	<br />
	
	 <div class="publication">
            <img src="logo/ACMMM22_frequency.png" onmouseover="this.src='logo/ACMMM22_frequency.png';" onmouseout="this.src='logo/ACMMM22_frequency.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Adaptively Learning Low-high Frequency Information Integration for Pan-sharpening</a>
                </strong>
		  <br> 
		<em><b>ACM MM, 2022</b></em>
                <br> 
               Man Zhou, Jie Huang, <b>Chongyi Li<sup>+</sup></b>, Hu Yu, Keyu Yan, Naishan Zheng, and Feng Zhao.
                <br>
                <span class="links">
                    <a href="">PDF</a>| 
		    <a href="">Project Page</a>| 	
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	 <div class="publication">
            <img src="logo/ACMMM22_normalization.png" onmouseover="this.src='logo/ACMMM22_normalization.png';" onmouseout="this.src='logo/ACMMM22_normalization.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Normalization-based Feature Selection and Restitution for Pan-sharpening</a>
                </strong>
		  <br> 
		<em><b>ACM MM, 2022</b></em>
                <br> 
               Man Zhou,  Jie Huang, Keyu Yan, Gang Yang, Aiping Liu, <b>Chongyi Li</b>, and Feng Zhao.
                <br>
                <span class="links">
                    <a href="">PDF</a>| 
		    <a href="">Project Page</a>| 	
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	

	 <div class="publication">
            <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="logo/HLRP-8M.mp4"> <source src="logo/HLRP-8M.mp4" type="video/mp4"></video>
            <p> 
                <strong>
                    <a href="">Underwater Image Enhancement with Hyper-Laplacian Reflectance Priors</a>
                </strong>
		  <br> 
		<em><b>TIP, 2022 <a href="" target="_blank"><font color="#ff0000">[TIP Popular Articles]</font></a></b></em>
                <br> 
               Peixian Zhuang, Jiamin Wu, Fatih Porikli, and <b>Chongyi Li<sup>+</sup</b>.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9854113">PDF</a>| 
		    <a href="https://github.com/zhuangpeixian/HLRP">Project Page</a>| 	
                    <a href="https://github.com/zhuangpeixian/HLRP">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />


	 <div class="publication">
            <video  muted="muted" width="300" class="publogo"   controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="MLLE_files/videos/underwater_video_demo.mp4"> <source src="MLLE_files/videos/underwater_video_demo.mp4" type="video/mp4"></video>
            <p> 
                <strong>
                    <a href="">Underwater Image Enhancement via Minimal Color Loss and Locally Adaptive Contrast Enhancement</a>
                </strong>
		  <br> 
		<em><b>TIP, 2022 <a href="" target="_blank"><font color="#ff0000">[TIP Popular Articles]</font></a></b></em>
                <br> 
               Weidong Zhang, Peixian Zhuang, Haihan Sun, Guohou Li, Sam Kwong, and <b>Chongyi Li<sup>+</sup</b>.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9788535">PDF</a>| 
		    <a href="https://li-chongyi.github.io/proj_MMLE">Project Page</a>| 	
                    <a href="https://github.com/Li-Chongyi/MMLE_code">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />

<div class="publication">
           <img src="logo/CIR-Net_logo.JPG" onmouseover="this.src='logo/CIR-Net_logo.JPG';" onmouseout="this.src='logo/CIR-Net_logo.JPG';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">CIR-Net: Cross-modality Interaction and Refinement for RGB-D Salient Object Detection</a>
                </strong>
		  <br> 
		<em><b>TIP, 2022</b></em>
                <br> 
              Runmin Cong, Qinwei Lin, Chen Zhang, <b>Chongyi Li<sup>+</sup></b>, Xiaochun Cao, Qingming Huang, and Yao Zhao.
                <br>
                <span class="links">
                    <a href="">PDF</a>| 
		    <a href="https://rmcong.github.io/proj_CIRNet.html">Project Page</a>| 	
                    <a href="https://github.com/rmcong/CIRNet_TIP2022">Code</a>	
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
		 
<div class="publication">
           <img src="logo/TCYB2022 _input.jpg" onmouseover="this.src='logo/TCYB2022_output.jpg';" onmouseout="this.src='logo/TCYB2022 _input.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Global-and-Local Collaborative Learning for Co-Salient Object Detection</a>
                </strong>
		  <br> 
		<em><b>TCYB, 2022</b></em>
                <br> 
              Runmin Cong, Ning Yang, <b>Chongyi Li<sup>+</sup></b>, Huazhu Fu, Yao Zhao, Qingming Huang, and Sam Kwong.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2204.08917.pdf">PDF</a>| 
		    <a href="https://rmcong.github.io/proj_GLNet.html/">Project Page</a>| 	
                    <a href="https://github.com/rmcong/GLNet_TCYB2022">Code</a>	
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	
	
	  <div class="publication">
            <img src="./logo/logo_TCSVT20221.jpg" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Underwater Image Enhancement Quality Evaluation: Benchmark Dataset and Objective Metric</a>
                </strong>
		  <br> 
		<em><b>TCSVT, 2022 <a href="" target="_blank"><font color="#ff0000">[TCSVT Popular Articles]</font></a></b></em>
                <br> 
              Qiuping Jiang, Yuses Gu, <b>Chongyi Li</b>, Runmin Cong, and Feng Shao.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9749233">PDF</a>| 
		    <a href="hhttps://github.com/yia-yuese/SAUD-Dataset">Code</a>| 
		    <a href="hhttps://github.com/yia-yuese/SAUD-Dataset">Dataset</a> 
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	
	  <div class="publication">
            <img src="./logo/UDC_Curve.jpg" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Under-Display Camera Image Enhancement via Cascaded Curve Estimationc</a>
                </strong>
		  <br> 
		<em><b>TIP, 2022</b></em>
                <br> 
              Jun Luo, Wenqi Ren, Tao Wang,  <b>Chongyi Li</b>, and Xiaochun Cao.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/document/9798712">PDF</a> 
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	  <div class="publication">
            <img src="./logo/survey.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Low-Light Image and Video Enhancement Using Deep Learning: A Survey</a>
                </strong>
		  <br> 
		<em><b>TPAMI, 2021</b></em>
                <br> 
               <b>Chongyi Li</b>, Chunle Guo, Linghao Han, Jun Jiang, Ming-Ming Cheng, Jinwei Gu, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2104.10729">PDF</a>| 
		    <a href="https://www.mmlab-ntu.com/project/lliv_survey/index.html">Project Page</a>| 	
                    <a href="https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open/">Collection</a>|
		    <a href="https://drive.google.com/file/d/1QS4FgT5aTQNYy-eHZ_A89rLoZgx_iysR/view?usp=sharing/">Dataset</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	  <div class="publication">
            <a href="https://imgsli.com/MTI4ODIz"><img src="logo/UDC.jpg" onmouseover="this.src='logo/UDC.jpg';" onmouseout="this.src='logo/UDC.jpg';" class="publogo"  width="300 px"></a>
            <p> 
                <strong>
                    <a href="">Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network</a>
                </strong>
		 <br> 
		<em><b>CVPR, 2021</b></em>
                <br> 
               Ruicheng Feng, <b>Chongyi Li</b>, Huaijin Chen, Shuai Li, Chen Change Loy, Jinwei Gu.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2104.09556">PDF</a>| 
                    <a href="https://jnjaby.github.io/projects/UDC/">Project Page</a>| 
                    <a href="https://github.com/jnjaby/DISCNet">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	  <div class="publication">
            <img src="./logo/zerodce++.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/9369102">Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation</a>
                </strong>
		  <br>
		<em><b>TPAMI, 2021 <a href="" target="_blank"><font color="#ff0000">[TPAMI Popular Articles]</font></a></b></em>
                <br>
               <b>Chongyi Li</b>, Chunle Guo, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9369102">PDF</a>| 
                    <a href="https://li-chongyi.github.io/Proj_Zero-DCE++.html">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/Zero-DCE_extension">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
     <div class="publication">
            <img src="./logo/UColor_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Underwater Image Enhancement via Medium Transmission-Guided Multi-Color Space Embedding</a>
                </strong>
		  <br> 
		<em><b>TIP, 2021 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper & TIP Popular Articles]</font></a></b></em>
                <br> 
               <b>Chongyi Li</b>, Saeed Anwar, Junhui Hou, Runmin Cong, Chunle Guo, Wenqi Ren.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9426457">PDF</a>| 
                    <a href="https://li-chongyi.github.io/Proj_Ucolor.html">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/Ucolor">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
		 
        <div class="publication">
            <img src="./logo/ZeroDCE_result_logo1.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf">Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement</a>
                </strong>
		  <br>
		<em><b>CVPR, 2020</b></em>
                <br>
                Chunle Guo, <b>Chongyi Li</b>,  Jichang Guo, Chen Change Loy,  Junhui Hou,  Sam Kwong,  Runmin Cong.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf">PDF</a>| 
                    <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/Zero-DCE">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/ECCV20-visual_results.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</a>
                </strong>
		  <br>
		<em><b>ECCV, 2020</b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Yongri Piao, Qianqian Xu, Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222.pdf">PDF</a>| 
                    <a href="https://li-chongyi.github.io/Proj_ECCV20">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/cmMS-ECCV20">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/ACMMM_framework.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://dl.acm.org/doi/10.1145/3394171.3413928">NuI-Go: Recursive Non-Local Encoder-Decoder Network for Retinal Image Non-Uniform Illumination Removal</a>
                </strong>
		   <br>
		<em><b>ACM MM, 2020</b></em>
                <br>
                <b>Chongyi Li</b>, Huazhu Fu, Runmin Cong, Zechao Li, Qianqian Xu.
                <br>
                <span class="links">
                    <a href="https://dl.acm.org/doi/10.1145/3394171.3413928">PDF</a>| 
                    <a href="Proj_ACMMM20_NuI-Go.html">Project Page</a>| 
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/benchmark_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8917818">An Underwater Image Enhancement Benchmark Dataset and Beyond</a>
                </strong>
		    <br>
		 <em><b>TIP, 2020 <a href="" target="_blank"><font color="#ff0000">[ESI Hot Paper & ESI Highly Cited Paper & TIP Popular Articles]</font></a></b></em>
                <br>
                <b>Chongyi Li</b>, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, Dacheng Tao.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8917818">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_benchmark.html">Project Page</a>| 
		    <a href="https://drive.google.com/file/d/12W_kkblc2Vryb9zHQ6BfGQ_NKUfXYk13/view">Benchmark</a>| 
                    <a href="https://github.com/Li-Chongyi/Water-Net_Code">Code</a>
                </span>
           </p>
        </div>
        <br>
        <br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/tgars_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8793227">Nested Network With Two-Stream Pyramid for Salient Object Detection in Optical Remote Sensing Images</a>
                </strong>
		     <br>
		 <em><b>TGRS, 2020 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper]</font></a></b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Junhui Hou, Sanyi Zhang, Yue Qian, Sam Kwong.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8793227">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_optical_saliency.html">Project Page</a>| 
		    <a href="https://drive.google.com/file/d/15FOnRo1xnz05fcNkXBhWy8WL0C26i8y4/view">Benchmark</a>| 
                    <a href="https://drive.google.com/file/d/1nnZKphu9_4oBvie4yqdxG95tYZRsqj4W/view">Results</a>
                </span>
           </p>
        </div>
        <br>
        <br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/PR_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://www.sciencedirect.com.remotexs.ntu.edu.sg/science/article/pii/S0031320319303401">Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement</a>
                </strong>
		     <br>
		 <em><b>PR, 2020 <a href="./PDF/PR_honorable mention_2020.pdf" target="_blank"><font color="#ff0000">[Pattern Recognition Best Paper Honourable Mention & ESI Highly Cited Paper]</font></a></b></em>
                <br>
                <b>Chongyi Li</b>, Saeed Anwar, Fatih Porikli.
                <br>
                <span class="links">
                    <a href="https://www.sciencedirect.com.remotexs.ntu.edu.sg/science/article/pii/S0031320319303401">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html">Project Page</a>| 
                    <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html">Code</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/TC2020_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/stamp/stamp.jsp?tp=&arnumber=8998588">ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D Salient Object Detection</a>
                </strong>
		     <br>
		<em><b>TCYB, 2020 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper & TCYB Popular Articles]</font></a></b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Sam Kwong, et al.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/stamp/stamp.jsp?tp=&arnumber=8998588">PDF</a>| 
                    <a href="https://github.com/Li-Chongyi/ASIF-Net">Code and Results</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/NeurIPS2020_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://proceedings.neurips.cc/paper/2020/file/4dc3ed26a29c9c3df3ec373524377a5b-Paper.pdf">CoADNet: Collaborative Aggregation-and-Distribution Networks for Co-Salient Object Detection</a>
                </strong>
		     <br>
		 <em><b>NeurIPS, 2020</b></em>
                <br>
                Qijian Zhang, Runmin Cong, Junhui Hou, <b>Chongyi Li</b>, and Yao Zhao.
                <br>
                <span class="links">
                    <a href="https://proceedings.neurips.cc/paper/2020/file/4dc3ed26a29c9c3df3ec373524377a5b-Paper.pdf">PDF</a>| 
                    <a href="https://rmcong.github.io/proj_CoADNet.html">Project Page</a>|
		    <a href="https://github.com/rmcong/CoADNet_NeurIPS20">Code</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/TIP_RS2020_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292434">Dense attention fluid network for salient object detection in optical remote sensing images</a>
                </strong>
		     <br>
		 <em><b>TIP, 2020 <a href="" target="_blank"><font color="#ff0000">[ESI Hot Paper &  ESI Highly Cited Paper]</font></a></b></em>
                <br>
                Qijian Zhang, Runmin Cong, <b>Chongyi Li</b>, Ming-Ming Cheng, Yuming Fang, Xiaochun Cao, Yao Zhao, and Sam Kwong.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292434">PDF</a>| 
                    <a href="https://rmcong.github.io/proj_DAFNet.html">Project Page</a>| 
                    <a href="https://github.com/rmcong/EORSSD-dataset">Benchmark</a>|
		    <a href="https://github.com/rmcong/DAFNet_TIP20">Code</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
	
        <div class="publication">
            <img src="./logo/depth_SR_logo.png" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8579111">Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution</a>
                </strong>
		     <br>
		<em><b>TIP, 2019</b></em>
                <br>
                Chunle Guo, <b>Chongyi Li<sup>+</sup></b>, Jichang Guo, Runmin Cong, Huazhu Fu, Ping Han.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8579111">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_SR.html">Project Page</a>| 
                    <a href="https://drive.google.com/file/d/18y6jpGnjqYINzMFJeVHcg4SQiYM-lFBb/view">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br />
	 <br />	
	<!--<
        <div class="publication">
            <img src="./logo/TMM19_logo.png" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8792133">PDR-Net: Perception-Inspired Single Image Dehazing Network With Refinement</a>
                </strong>
		     <br>
		<em><b>TMM, 2019</b></em>
                <br>
                <b>Chongyi Li</b>, Chunle Guo, Jichang Guo, Ping Han, Huazhu Fu, Runmin Cong.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8792133">PDF</a>| 
                    <a href="">Project Page</a>| 
                    <a href="">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br />
	 <br />	
	-->
	
        <div class="publication">
            <img src="./logo/TIP16_logo.png" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/7574330">Underwater Image Enhancement by Dehazing with Minimum Information Loss and Histogram Distribution Prior</a>
                </strong>
		     <br>
		 <em><b>TIP, 2016 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper & TIP Popular Articles]</font></a></b></em>
                <br>
                <b>Chongyi Li</b>, Jichang Guo, Runmin Cong, et al.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/7574330">PDF</a>| 
                    <a href="">Project Page</a>| 
                    <a href="https://github.com/Li-Chongyi/TIP2016-code">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
</div>
<br>
<hr />



<!--<
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Participating Fundings:</h2>

<ul>
<li>"Study on the hierarchical modeling of underwater imaging and underwater image/video clearness method", The National Natural Science Foundation of China (NSFC). (Principal Participator) 
</ul>
<br>
<hr />
-->


<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Events:</h2>  

<div class="publication">
            <img src="logo/codeformer1.jpg" onmouseover="this.src='logo/codeformer2.jpg';" onmouseout="this.src='logo/codeformer1.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Try Your Face (face restoration platform)</a>
                </strong> 
              We provide our code of CodeFormer (Towards Robust Blind Face Restoration with Codebook Lookup TransFormer) on Colab. Please feel free to try your face.
                <br>
                <span class="links">
		      <a href="https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing">Colab</a>	
		      
                </span>
            </p>
          </div>
          <br>
	  <br>
	<br />
	<br />
	<br />
	
<div class="publication">
            <img src="logo/plateform1.jpg" onmouseover="this.src='logo/plateform2.jpg';" onmouseout="this.src='logo/plateform1.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Low-Light Image Enhancement Online Platform</a>
                </strong> 
              Different algorithms demand various configurations, GPU versions, and hardware specifications that are prohibitive to beginners who are new to this area and may not even have GPU resources. We contribute an online plateform.
                <br>
                <span class="links">
		      <a href="http://mc.nankai.edu.cn/ll/">Website</a>	
		      
                </span>
            </p>
          </div>
          <br>
	  <br>
	<br />
	<br />
	<br />
	
	  <div class="publication">
           <img src="./logo/MIPI.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Mobile Intelligent Photography & Imaging</a>
                </strong>
		  <br> 
		<em><b>1st MIPI workshop @ ECCV 2022</b></em>
                <br> 
              <b>Chongyi Li</b>, Shangchen Zhou, Ruicheng Feng, Jun Jiang, Wenxiu Sun, Qingyu Yang, Qingpeng Zhu, Chen Change Loy, and Jinwei Gu.
                <br>
                <span class="links">
		      <a href="http://mipi-challenge.org/">Website</a>	
		      
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
<ul>
<li><a href="http://mipi-challenge.org/" target="_blank"><font color="#A52A2A">[Workshop]</font></a> ECCV 2022 Workshop on Mobile Intelligent Photography and Imaging (MIPI)</a></li>
<li><a href="https://attend.ieee.org/mmsp-2022/special-sessions/underwater-multimedia-processing/" target="_blank"><font color="#A52A2A">[Special Session]</font></a> IEEE MMSP 2022 Special Session on Underwater Multimedia Processing</a></li>
<li><a href="https://www.frontiersin.org/research-topics/39049/multimodal-intelligence" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> Frontiers in Signal Processing Special Issue on Multimodal Intelligence</a></li>
<li><a href="https://ieeeoes.org/wp-content/uploads/2021/07/JOE_cfp_AMLM.pdf" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> IEEE Journal of Oceanic Engineering Special Issue on Advanced Machine Learning Methodologies for Underwater Image and Video Processing and Analysis (2021-2022)</a></li>
<li><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/MTAP_SI_CFP.pdf" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> Multimedia Tools and Applications Special Issue on Depth-Related Processing and Applications in Visual Systems (2020-2021)</a></li>
<li><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/SPIC_SI_cfp.pdf" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> Signal Processing : Image Communication Special Issue on Visual Information Processing for Underwater Images and Videos: Theories, Algorithms, and Applications (2019-2020)</a></li>
<li><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/APSIPA-ASC-2019-CfP.pdf" target="_blank"><font color="#A52A2A">[Special Session]</font></a> APSIPA ASC 2019 Special Session on Multi-source Data Processing and Analysis: Models, Methods and Applications (2019-2020)</a></li>

</ul>
<br>
<hr />

	
	
 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Honors & Awards:</h2>

<ul>
<li>Pattern Recognition (Elsevier Journal) <a href="./PDF/PR_honorable mention_2020.pdf">(Best Paper Honourable Mention)</a></li>
<li> Our team Feedforward won the <strong>second runner-up (3/317)</strong> in <a href="https://studio.brainpp.com/competition/5?name=2022%20MegCup%20%E7%82%BC%E4%B8%B9%E5%A4%A7%E8%B5%9B&tab=rank" target="_blank"><font color="#ff0000">MegCup Efficient Model for Blind Denoising</font></a>. Congrats to Xin Jin, Ruiqi Wu, and Zhen Li. The code is publicly available <a href="https://github.com/Srameo/megcup-feedforward" target="_blank"><font color="#ff0000">here.</font></a></li>
<li><strong>World‚Äôs Top 2% Scientists (2022)</strong>. It is compiled by Stanford University based on the standardized citation indicators (Table_1_Authors_singleyr_2021_pubs_since_1788_wopp_extracted_202209), which is avaiable online at <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/4" target="_blank"><font color="#ff0000">Mendeley Database.</font></a></li>
<li><strong>World‚Äôs Top 2% Scientists (2021)</strong>. It is compiled by Stanford University based on the standardized citation indicators (Table_1_Authors_singleyr_2020_wopp_extracted_202108), which is avaiable online at <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/3" target="_blank"><font color="#ff0000">Mendeley Database.</font></a></li>
<!--<<li>2021 Innovation in Information Technology Application and Artificial Intelligent Academic Forum for Post-doctoral Talents, Tianjin, China. Runners-up Award. <a href="PDF/postdoc_runner-up.jpg"  target="_blank">PDF</a></li>-->
<li>ECCV (Outstanding Reviewer in 2022). <a href="https://eccv2022.ecva.net/program/outstanding-reviewers/"  target="_blank">PDF</a></li>
<li>ICCV (Outstanding Reviewer in 2021). <a href="http://iccv2021.thecvf.com/outstanding-reviewers"  target="_blank">PDF</a></li>
<li>CVPR (Outstanding Reviewer in 2021). <a href="http://cvpr2021.thecvf.com/node/184"  target="_blank">PDF</a></li>
<li>IEEE Journal of Oceanic Engineering (Outstanding Reviewer in 2021) <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9756533">News</a></li>
<!--<<li>6th, NTIRE 2021 Challenge on Non-Homogeneous Image Dehazing, 2021. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/NTIRE2021_NonHomogeneous_Dehazing_Report_compressed.pdf" target="_blank">PDF</a></li>-->
<!--<<li>2020 National Postdoctoral Forum on the Development and Application of Artificial Intelligence, Tianjin, China. Excellence Award. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/%E5%8D%9A%E5%90%8E%E4%BC%98%E7%A7%80%E5%A5%96.pdf">PDF</a></li>-->
<li>Distinguished Dissertation Award of Beijing Society of Image and Graphics, 2018. <a href="http://www.bsig.org.cn/detail/2316">Media</a></li> 

<!--<li>Bohai Securities Fellowship, 2017.</li>
<li>"Outstanding Graduate Student" in Tianjin University, 2018.
<li>"Merit Student" in Tianjin University, 2017.</li> 
<li>"Advanced Individual" in the creative working, 2017.</li>
<li>China Scholarship Council (CSC) scholarships, 2016.</li>
<li>The First Class Academic Scholarship in Tianjin University, 2015, 2016.</li><embed src="https://sumanbogati.github.io/sample.pdf" type="application/pdf" />
<li>"Advanced Individual" in international exchange, 2015.</li>
<li>"Advanced Individual" in the creative working, 2015.</li>-->

</ul>
<br>
<hr /> 


 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Professional Service:</h2>

<ul>
<br><strong>Associate Editor</strong></br>
2022/05- : IEEE Journal of Oceanic Engineering (IF: 3.554)</br>
2022/01- : Neurocomputing (IF: 5.719)</br>
2021/01- : Springer Journal of Signal, Image and Video Processing (IF: 1.794)</br> 

<br><strong>Guest Editor</strong></br>
IEEE Journal of Oceanic Engineering (Lead Guest Editor) (IF: 3.554)</br>
Signal Processing: Image Communication (Management Guest Editor) (IFÔºö2.779)</br>
Multimedia Tools and Applications (IF: 2.313)</br>
Frontiers in Signal Processing</br>

<br><strong>Area Chair</strong></br>
BMVC 2022</br>

<br><strong>Co-organizer</strong></br>
ECCV Workshop on Mobile Intelligent Photography & Imaging (MIPI) 2022</br>
IEEE Workshop on Multimedia Signal Processing (MMSP) 2022</br>
<!--Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC) 2019</br>-->

<br><strong>Senior Program Committee (Meta-Reviewers)</strong></br>
AAAI 2022, 2023</br>
<!--
<br><strong>Reviewer</strong></br>
<li><strong>IEEE</strong>:</li>TPAMI, TIP, TCyber, TCSVT, TNNLS, TMM, TB, TIM, TGARS, TITS, TCSVT II: Express Briefs, ACCESS, SPL, JOE, JBHI, J-STARS, MultiMedia

<li><strong>Elsevier</strong>:</li> CVIU, JOE, DSP, Neurocomputing, PRL; SPIC, JCG; Knowledge-based Systems 

<li><strong>Springer</strong>:</li> IJCV, VC, SCIS,  MVA,  EURASIP JIVP

<li><strong>ACM</strong>:</li> Computing Surveys

<li><strong>IET</strong>:</li> CV

<li><strong>SPIE</strong>:</li> JEI

<li><strong>Conference</strong>:</li>ICPR 2018; ACM MM 2019;ICME 2020; ACM MM2020; MICCAI 2020; AAAI 2021; CVPR 2021; IJCAI 2021;ICME 2021; MICCAI 2021; ACM MM2021; ICCV2021; NeurIPS2021; ICLR2022; CVPR2022
-->

</ul>
<br>
<hr />    



 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Supervision and Teaching:</h2>
<ul>
<li>Co-Supervisor, Ph.D.: Haoying Li (Zhejiang University),  Visiting Scholar: NeRF Editing, NTU, 08/2022-present</li>
<li>Co-Supervisor, M.E.: Jixin Zhao (Tianjin University),  Intern: Video Denoising, NTU, 08/2022-present</li>
<li>Co-Supervisor, B.E.: Yihang Luo (Nanyang Technological University),  Intern: Image Colorization, NTU, 08/2022-present</li>	
<li>Co-Supervisor, M.E.: Zhexin Liang (Zhejiang University),  AI Major Project: Backlit Image Enhancement, NTU, 01/2022-present</li>	
<li>Co-Supervisor, M.E.: Yuekun Dai (Peking University),  AI Major Project: Nighttime Flare Removal, NTU, 08/2021-present</li>
<li>Co-Supervisor, M.E.: Xingshu Wang (Beijing University of Posts and Telecommunications),  Project: Reference-Based Image Super-Resolution, ANU</li>

<li>Teaching Assistant, NTU CE6126: MSAI Advanced Computer Vision, NTU, Fall 2020 </li>
</ul>
<br>
<hr /> 


 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Alumni:</h2>
<ul>
<li>Co-Supervised with Cavan, Zurang Liu (Beijing University of Posts and Telecommunications), Project: Deep Image Harmonization, AI Master@NTU, 01/2021-04/2022, Now: Engineer@ByteDance, SG</li>
<li>Co-Supervisor with Cavan, Sihao Chen, Project: Learning to See in the Dark <a href="https://drive.google.com/file/d/12Vu-n2Kw3DV3qa6VfOVTpbyy8TGfRIQz/view?usp=sharing">[Final Report--Video (A+)]</a>, FYP of Computer Science@NTU, 06/2020-06/2021, Now: Engineer@Shopee, SG. Sihao's project is awarded the <strong>2021 Global Undergraduate Awards</strong> for his entry 'Learning to See in the Dark - Low Light Image Enhancement'.  <a href="https://undergraduateawards.com/" target="_blank"><font color="#ff0000">[The Global Undergraduate Awards]</font></a><a href="https://www.ntu.edu.sg/scse/news-events/news/detail/scse-s-chen-sihao-(computer-science-year-4)-makes-a-historical-first-win-at-the-global-undergraduate-awards-2021" target="_blank"><font color="#ff0000">[NTU News]</font></a></li>
<li>Co-Superviso with Cavan, Qiming Ai (University of Science and Technology of China),  Project: Deep Photo Enhancement,  AI Master@NTU, 01/2020-12/2020, Now:</li>
</ul>
<br>
<hr /> 
  
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Miscellaneous:</h2>

 
<ul>
<li><a href="https://unsplash.com/"><font color="#1C86EE">Unsplash</font></a></li>
<li><a href="https://pngtree.com/"><font color="#1C86EE">Pngtree</font></a></li>
<li><a href="https://www.wordclouds.com/"><font color="#1C86EE">WordClouds</font></a></li>
<li><a href="https://emojipedia.org/"><font color="#1C86EE">Emojipedia</font></a></li>
<li><a href="https://film-grab.com/"><font color="#1C86EE">FilmGrab</font></a></li>
<li><a href="https://deviparikh.medium.com/how-we-write-rebuttals-dc84742fece1/"><font color="#1C86EE">How we write rebuttals</font></a></li>
<li><a href="http://www-net.cs.umass.edu/kurose/writing/intro-style.html"><font color="#1C86EE">Writing a good introduction</font></a></li>
<li><a href="https://www.computer.org/publications/tech-news/trends/deep-learning-vs-machine-learning-whats-the-difference?source=cssocial"><font color="#1C86EE">Deep Learning vs Machine Learning: What‚Äôs the Difference</font></a></li>
</ul>
<br>


<!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=pPHWAkKgmzsFC_v7-3ndOuL5q3qL_EhEE16zTJwxtRw"></script> -->
<!--<div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3244445&c=9733648" alt="AmazingCounters.com"></a></div>--> 
<!--div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3230662&c=9692299" alt="AmazingCounters.com"></a></div>-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-156698907-1"></script>-->




</section>

</div>
<!--<script src="javascripts/scale.fix.js"></script>-->
</body>
</html>  
