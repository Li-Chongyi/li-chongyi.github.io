<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Projects</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
  </head>
  <body>
    <div class="wrapper">
<header>
<h7>Chongyi Li</h7><br><br>
<div>

<img src="sub_img/lichongyi_photo.jpg" border="0" width="80%"><br></div><br>
  
<p>
<small>lichongyi25@gmail.com lichongyi@tju.edu.cn</small><br><br>
<a href="https://github.com/Li-Chongyi" target="_blank">[GitHub]</a>  
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chongyi" target="_blank">[DBLP]</a>  <br>
<a href="https://scholar.google.com/citations?user=1_I0P-AAAAAJ&hl=zh-CN" target="_blank">[Google Scholar]</a> <br>
</p> <br>
<p class="view"><a href="https://li-chongyi.github.io/">Homepage</a></p>
<p class="view"><a href="sub_publication.html">Publications</a></p>
<p class="view"><a href="sub_projects.html">Projects</a></p>
</header>

      <section>

<h2>
<a id="publications-pages" class="anchor" href="#publications-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Projects:</h2>

<hr />

<h3>Emerging from Water: Underwater Image Color Correction Based on Weakly Supervised Color Transfer [<a href="proj_Emerging_water.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>We propose a weakly supervised color transfer method to correct color distortion, which relaxes the need for paired underwater images for training
and allows the underwater images being taken in unknown locations.</p> 
<br>

<hr />
 </div>
<h3>LightenNet: a convolutional neural network for weakly illuminated image enhancement [<a href="proj_lowlight.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>Weak illumination or low light image enhancement as pre-processing is needed in many computer vision asks. Existing methods show limitations when they are used to enhance weakly illuminated images,
especially for the images captured under diverse illumination circumstances. In this letter, we propose  trainable Convolutional Neural Network (CNN) for weakly illuminated image enhancement, namely
LightenNet, which takes a weakly illuminated image as input and outputs its illumination map that is ubsequently used to obtain the enhanced image based on Retinex model. The proposed method produces
visually pleasing results without over or under-enhanced regions. Qualitative and quantitative comparisons are conducted to evaluate the performance of the proposed method. The experimental results
demonstrate that the proposed method achieves superior performance than existing methods. Additionally, we propose a new weakly illuminated image synthesis approach, which can be use as a guide for
weakly illuminated image enhancement networks training and full-reference image quality assessment.</p>   
<br>

<hr />
 
  
<h3>Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution [<a href="proj_SR.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
 <p>Rapid development of affordable and portable consumer depth cameras facilitates the use of depth information in many computer vision tasks such as intelligent vehicles and 3D reconstruction. 
  However, depth map captured by low-cost depth sensors (e.g., Kinect) usually suffers from low spatial resolution, which limits its potential applications. In this paper, we propose a novel 
  deep network for depth map super-resolution (SR), called DepthSR-Net. The proposed DepthSR-Net automatically infers a high resolution (HR) depth map from its low resolution (LR) version by 
  hierarchical features driven residual learning. Specifically, DepthSR-Net is built on a residual U-Net deep network architecture. Given LR depth map, we first obtain the desired HR by bicubic 
  interpolation upsampling, and then construct an input pyramid to achieve multiple level receptive fields. Next, we extract hierarchical features from the input pyramid, intensity image, and 
  encoder-decoder structure of U-Net. Finally, we learn the residual between the interpolated depth map and the corresponding HR one using the rich hierarchical features. The final HR depth map 
  is achieved by adding the learned residual to the interpolated depth map. We conduct an ablation study to demonstrate the effectiveness of each component in the proposed network. Extensive 
  experiments demonstrate that the proposed method outperforms the state-of-the-art methods. Additionally, the potential usage of the proposed network in other low-level vision problems is
  discussed.</p> 
<br>

<hr />
 
  
<h3>An Underwater Image Enhancement Benchmark Dataset and Beyond [<a href="proj_benchmark.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robot. Numerous underwater image enhancement algorithms have been proposed in the last few years. 
However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge 
the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world degraded images. In this paper, we construct an 
Underwater Image Enhancement Benchmark Dataset (UIEBD) including 950 real-world underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory 
references as challenging data. Using this dataset, we conduct a comprehensive study of the state-of-the-art underwater image enhancement algorithms qualitatively and quantitatively. In addition, we propose an end-to-end Deep
Underwater Image Enhancement Network (DUIENet) trained on this benchmark as a baseline, which indicates the generalization of the proposed UIEBD for training Convolutional Neural Networks (CNNs). The benchmark evaluations and 
the proposed DUIENet demonstrate the performance and limitations of state-of-the-art algorithms which shed light on the future research in underwater image enhancement.</p>
<br>

<hr />
 
<h3>Nested Network with Two-Stream Pyramid for Salient Object Detection in Optical Remote Sensing Images [<a href="proj_optical_saliency.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>Arising from the various object types and scales, diverse imaging orientations, and cluttered backgrounds in optical remote sensing image (RSI), it is difficult to directly extend
the success of salient object detection for nature scene image to the optical RSI. In this paper, we propose an end-to-end deep network called LV-Net based on the shape of network
architecture, which detects salient objects from optical RSIs in a purely data-driven fashion. The proposed LV-Net consists of two key modules, i.e., a two-stream pyramid module (L-shaped
module) and an encoder-decoder module with nested connections (V-shaped module). Specifically, the L-shaped module extracts a set of complementary information hierarchically by using a two-
stream pyramid structure, which is beneficial to perceiving the diverse scales and local details of salient objects. The V-shaped module gradually integrates encoder detail features with decoder
semantic features through nested connections, which aims at suppressing the cluttered backgrounds and highlighting the salient objects. In addition, we construct the first publicly available
optical RSI dataset for salient object detection, including 800 images with varying spatial resolutions, diverse saliency types, and pixel-wise ground truth. Experiments on this benchmark
dataset demonstrate that the proposed method outperforms the state-of-the-art salient object detection methods both qualitatively and quantitatively.</p>
<br>

<hr />

<h3>Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement [<a href="proj_underwater_image_synthesis.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>In underwater scenes, wavelength-dependent light absorption and scattering degrade the visibility of images and videos. The degraded underwater images and videos affect the accuracy of pattern recognition, visual understanding, and key feature extraction in underwater scenes. In this paper, we propose an underwater image enhancement convolutional neural network (CNN) model based on underwater scene prior, called UWCNN. Instead of estimating the parameters of underwater imaging model, the proposed UWCNN model directly reconstructs the clear latent underwater image, which benefits from the underwater scene prior which can be used to synthesize underwater image training data. Besides, based on the light-weight network structure and effective training data, our UWCNN model can be easily extended to underwater videos for frame-by-frame enhancement. Specifically, combining an underwater imaging physical model with optical properties of underwater scenes, we first synthesize underwater image degradation datasets which cover a diverse set of water types and degradation levels. Then, a light-weight CNN model is designed for enhancing each underwater scene type, which is trained by the corresponding training data. At last, this UWCNN model is directly extended to underwater video enhancement. Experiments on real-world and synthetic underwater images and videos demonstrate that our method generalizes well to different underwater scenes.</p>
<br>

<hr />

<h3>Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement [<a href="Proj_Zero-DCE.html"><font color="#0000FF">Details</font></a>]</h3>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>The paper presents a novel method, Zero-Reference Deep Curve Estimation (Zero-DCE), which formulates light enhancement as a task of image-specific curve estimation with a deep network.  Our method trains a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order curves for dynamic range adjustment of a given image.The curve estimation is specially designed, considering pixel value range, monotonicity, and differentiability. Zero-DCE is appealing in its relaxed assumption on reference images, i.e., it does not require any paired or unpaired data during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and drive the learning of the network. Our method is efficient as image enhancement can be achieved by an intuitive and simple nonlinear curve mapping. Despite its simplicity, we show that it generalizes well to diverse lighting conditions. Extensive experiments on various benchmarks demonstrate the advantages of our method over state-of-the-art methods qualitatively and quantitatively. Furthermore, the potential benefits of our Zero-DCE to face detection in the dark are discussed.</p>
<br>

<hr />
      </section>

    </div>
    <script src="../javascripts/scale.fix.js"></script>
  </body>
</html>
