<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement</title>
  <!--=================Meta tags==========================-->
  <meta name="robots" content="index,follow">
  <meta name="keywords" content="Low-light, UHD, Image Enhancement">
  <link rel="author" href="https://li-chongyi.github.io//">
  <!--=================js==========================-->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link href="./css.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="./project.css" media="screen">
  <script src="./effect.js "></script>
  <!-- Latex -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
  <script type="text/javascript" async src="./canvas-nest-1.0.1.min.js"></script>

  <!--=================Google Analytics==========================-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129775907-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-129775907-1');
  </script>
</head>

<body>
  <div id="content">
    <div id="content-inner" >
      <div class="section head" style="z-index: 1;">
          <br>
        <h1>
          Embedding<font color="#FF0000">Four</font>ier for <font color="#FF0000">U</font>ltra-<font color="#FF0000">H</font>igh-<font color="#FF0000">D</font>efinition
          <br> Low-Light Image Enhancement
        </h1>
       <!--  <h1>
        </h1> -->
        <!--=================Authors==========================-->
        <div class="authors";">
          <br>
          <a href="https://li-chongyi.github.io" target="_blank">Chongyi Li</a>&nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://scholar.google.com.au/citations?user=RZLYwR0AAAAJ&hl=en" target="_blank">Chun-Le Guo</a> 
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://manman1995.github.io/" target="_blank">Man Zhou</a> 
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href=" " target="_blank">Zhexin Liang</a> 
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://shangchenzhou.com/" target="_blank">Shangchen Zhou</a> 
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://jnjaby.github.io/" target="_blank">Ruicheng Feng</a> 
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://www.mmlab-ntu.com/person/ccloy/" target="_blank">Chen Change Loy</a> 

          
        </div>

        <div class="affiliations ">
          S-Lab, Nanyang Technological University &nbsp;&nbsp;&nbsp;&nbsp; <br>Nankai University&nbsp;&nbsp;&nbsp;&nbsp;<br>
        </div>
        <!--=================Tabs==========================-->
        <ul id="tabs">
          <li><a href="#abstract" name="#tab1">Abstract</a></li>
          <li><a href="#dataset" name="#tab3">Dataset</a></li>
          <li><a href="#method" name="#tab3">Method</a></li>
          <li><a href="#results" name="#tab4">Results</a></li>
          <li><a href="#citation" name="#tab5">Citation</a></li>
      </div>
      <br>

<!--       <div style="text-align: center;">
        <video width="700", poster="src/video_poster.png" controls><source src="src/video.mp4" type=video/mp4></video>
      </div> -->



      <!--=================Abstract==========================-->
      <div class="section abstract", id="abstract">
        <h2>Abstract</h2>
        <p style="text-align:justify; text-justify:inter-ideograph;"> 
          <br>
          Ultra-High-Definition (UHD) photo has gradually become the standard configuration in advanced imaging devices. The new standard unveils many issues in existing approaches for low-light image enhancement (LLIE), especially in dealing with the intricate issue of joint luminance enhancement and noise removal while remaining efficient. Unlike existing methods that address the problem in the spatial domain, we propose a new solution, <strong>UHDFour</strong>, that embeds Fourier transform into a cascaded network. Our approach is motivated by a few unique characteristics in the Fourier domain:  1) most luminance information concentrates on amplitudes while noise is closely related to phases, and 2) a high-resolution image and its low-resolution version share similar amplitude patterns. Through embedding Fourier into our network, the amplitude and phase of a low-light image are separately processed to avoid amplifying noise when enhancing luminance.  Besides, UHDFour is scalable to UHD images by implementing amplitude and phase enhancement under the low-resolution regime and then adjusting the high-resolution scale with few computations. We also contribute the first real UHD LLIE dataset, <strong>UHD-LL</strong>, that contains 2,150 low-noise/normal-clear 4K image pairs with diverse darkness and noise levels captured in different scenarios. With this dataset, we systematically analyze the performance of existing LLIE methods for processing UHD images and demonstrate the advantage of our solution. We believe our new framework, coupled with the dataset, would push the frontier of LLIE towards UHD.  </p>
          <br>
       </div>

      <!--=================Dataset==========================-->
      <div class="section" , id="dataset">
        <h2>Dataset</h2>
        <div style="text-align: center; vertical-align:middle">
          <img src="src/UHD_LL_dataset.png" width="800">
        </div>
        <div align="center" style="margin-top: 5px">
          <b>UHD-LL Dataset</b>
        </div>
        <p style="text-align:justify; text-justify:inter-ideograph;"> 
          We collect a real low-noise/normal-clear paired image dataset that contains 2,150 pairs of 4K UHD data saved in 8bit sRGB format. Images are collected from a camera mounted on a tripod to ensure stability. Two cameras, a Sony $\alpha7$ camera and a Sony Alpha a6300 camera, are used to offer diversity. The ground truth (or normal-clear) image is captured with a small ISO in [100,800] in a bright scene (indoor or outdoor). The corresponding low-noise image is acquired by increasing the ISO in [1000,20000] and reducing the exposure time. Due to the constraints of exposure gears in the cameras, shooting in the large ISO range may produce bright images, which opposes the purpose of capturing low-light and noisy images. Thus, in some cases, we put a neutral-density (ND) filter with different ratios on the camera lens to capture low-noise images. In this way, we can increase the ISO to generate heavier noises and simultaneously obtain extremely dark images, enriching the diversity of darkness and noise levels. 
        </p>
      </div>

      <!--=================Method==========================-->
      <div class="section" , id="method">
        <h2>Method</h2>
        <div style="text-align: center; vertical-align:middle">
          <img src="src/motivation.png" width="900">
        </div>
        <div align="center" style="margin-top: 5px">
          <b>Motivation</b>
        </div>
        <p style="text-align:justify; text-justify:inter-ideograph;"> 
          We observed that <strong>(a)</strong> luminance and noise can be `decomposed' to a certain extent in the Fourier domain and <strong>(b)</strong> HR image and its LR versions share similar amplitude patterns. The amplitude and phase are produced by Fast Fourier Transform (FFT) and the compositional images are obtained by Inverse FFT (IFFT). For visualization, we show the amplitude and phase in imagery format with common transformations.  Lines of the same color indicate a set of FFT/IFFT transforms. The red triangles mark the similar pattern (obviously different from the gray one). 
        </p>
        <div style="text-align: center; vertical-align:middle">
          <img src="src/framework.png" width="900">
        </div>
        <div align="center" style="margin-top: 5px">
          <b>Overview of UHDFour</b>
        </div>
        <p style="text-align:justify; text-justify:inter-ideograph;"> 
          Our approach consists of an LRNet and an HRDNet. The LRNet is an encoder-decoder network that produces  8$\times$ downsampled result $\hat{y_{8}}$ and the refined amplitude $A_{r}$ and phase $P_{r}$ features. We omit the skip connections for brevity. The HRNet contains an Adjustment Block and the upsampling operation, producing the final result $\hat{y}$. Most computation is conducted in the LRNet.
        </p>
      </div>

      <!--=================Results==========================-->
      <div class="section" , id="results">
        <h2>Results</h2>
         <div style="text-align: center; vertical-align:middle">
          <img src="src/study1.png" width="800">
        </div>
        <div align="center" style="margin-top: 5px">
          <b>Visual comparison between state of the arts for restoring a UHD low-light image.</b>
        </div>
        <p style="text-align:justify; text-justify:inter-ideograph;"> 
          We use the released model directly in this evaluation. All released models cannot handle the UHD low-light image well.
        </p>
      
        <div style="text-align: center; vertical-align:middle">
          <img src="src/comparison2.png" width="800">
        </div>
        <div align="center" style="margin-top: 5px">
          <b>Visual comparison between the retrained state of the arts on the UHD-LL dataset.</b>
        </div>
        <p style="text-align:justify; text-justify:inter-ideograph;"> 
          All compared models leave noise, artifacts, or color deviations in the results. Our method achieves a visually pleasing result.
        </p>

      <div class="section" id="Materials">
        <!--<h2>Meterials</h2>
         <table style="width:100%; text-align: center; margin-top: 20px; font-size: 16.5px;">
          <tbody style="width: 100%; margin-left: 40px; margin-right: 40px;">
            <tr>
              <td style="width:33%; text-align: center">
                <a href="https://arxiv.org/abs/2208.06857">
                  <img src="src/paper.png"/>
                </a>
              </td>
              <td style="width: 33%; text-align: center">
                <a href="https://pan.baidu.com/s/1K29p3gJWYa1ZM0vMHqI4uA">
                  <img src="src/cloud-download.png" style="height: 150px; width:150px"/>
                </a>
              </td>
              <td style="width:33%; text-align: center">
                <a href="https://github.com/RQ-Wu/UnderwaterRanker">
                  <img src="src/icon_github.png" style="height: 140px; width:140px"/>
                </a>
              </td>
            </tr>
            <tr>
              <td>
                <br>
                <a href="https://arxiv.org/abs/2208.06857">Paper</a>
              </td>
              <td>
                <br>
                <a href="https://pan.baidu.com/s/1K29p3gJWYa1ZM0vMHqI4uA">Dataset (pwd: nuin)</a>
              </td>
              <td>
                <br>
                <a href="https://github.com/RQ-Wu/UnderwaterRanker">Codes</a>
              </td>
            </tr>
          </tbody>
        </table>
      </div> -->

        <h2>Materials</h2>
        <br>
        <table width="100%" align="center" border=none cellspacing="0" cellpadding="30">
          <tr>
            <td width="30%">
              <center>
                <a href="https://arxiv.org/abs/2302.11831" target="_blank" class="imageLink"><img
                    src="./src/paper.png" , height="120"></a><br><br>
                <a href="https://arxiv.org/abs/2302.11831" disabled target="_blank">Paper</a>
              </center>
            </td>
            <td width="30%">
              <center>
                <a href="https://drive.google.com/drive/folders/1IneTwBsSiSSVXGoXQ9_hE1cO2d4Fd4DN" target="_blank" class="imageLink"><img
                    src="./src/cloud-download.png" , height="120"></a><br><br>
                <a href="https://drive.google.com/drive/folders/1IneTwBsSiSSVXGoXQ9_hE1cO2d4Fd4DN" target="_blank">Dataset</a>
              </center>
            </td>
            <td width="30%" valign="middle">
              <center>
                <a href="https://github.com/Li-Chongyi/UHDFour_code" target="_blank" class="imageLink"><img
                    src="./src/icon_github.png" , height="120"></a><br><br>
                <a href="https://github.com/Li-Chongyi/UHDFour_code" target="_blank">Code</a>
              </center>
            </td>
          </tr>
        </table>
      </div>

      <!--=================License==========================-->
      <div class="section" , id="License">
        <h2>License</h2>
        <p>
          We retain all the copyrights of this method.
        </p>
      </div> 


      <!--=================Citation==========================-->
      <div class="section citation" , id="citation">
        <h2>Citation</h2>
        <p>If you find our dataset and paper useful for your research, please consider citing our work:
        </p>
        <div class="section bibtex">
          <pre style="padding-left: 10px;">@inproceedings{UHDFourICLR2023,
  title={EmbeddingFourier for Ultra-High-Definition
Low-Light Image Enhancement},
  author={Chongyi Li and Chun-Le Guo and  Man Zhou  and Zhexin Liang and  Shangchen Zhou and Ruicheng Feng and Chen Change Loy},
  booktitle={ICLR},
  year={2023}
}</pre>
        </div>
      </div>

      <!--=================Contact==========================-->
      <div class="section contact">
        <h2 id="contact">Contact</h2>
        <p>If you have any question, please contact us via <strong>lichongyi25@gmail.com</strong>.
        </p>
        <br>
      </div>
</body>

</html>
