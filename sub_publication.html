<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<head>
 <link rel="Shortcut Icon" href="./logo/hp_logo.jpg" sizes=16x16  type="image/x-icon" />
 <link rel="Bookmark" href="./logo/hp_logo.jpg" sizes=16x16 type="image/x-icon" />
  
<title>Publications</title> 

<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<meta name="viewport" content="width=device-width">
</head>
<body>
<div class="wrapper">
<header>
<h7>Chongyi Li</h7><br><br>
<div>
<img src="sub_img/lichongyi_nankai.jpg" border="0" width="80%"><br></div><br>

  
<p>
<small>ðŸ“§lichongyi25 at gmail.com</small><br>
<small>ðŸ“§lichongyi at nankai.edu.cn</small><br><br>
<a href="https://pi-lab.xyz/index.html" target="_blank">[&pi; Research Group]</a>
 
</p> <br>
<p class="view"><a href="https://li-chongyi.github.io/">Homepage</a></p>
<p class="view"><a href="sub_publication.html">Publications</a></p>
</header>

<section>

<h2>
<a id="publications-pages" class="anchor" href="#publications-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications:</h2>
<div style="text-align: justify; display: block; margin-right: auto;">


  
 
 <h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preprint:</h4>

<ol>
<li>C. Li, C. Guo, R. Feng, etc, <strong>CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment</strong>. <a href="https://arxiv.org/pdf/2207.14273.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://li-chongyi.github.io/CuDi_files/" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> S. Anwar, M. Tahir, C. Li, et al., <strong>Image Colorization: A Survey and Dataset</strong>. <a href="https://arxiv.org/pdf/2008.10774.pdf" target="_blank"><font color="#1C86EE">[PDF--arXiv Version]</font></a><a href="https://github.com/saeed-anwar/ColorSurvey" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
</ol> 
  

<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2024:</h4>

<ol>
<li> Learning Inclusion Matching for Animation Paint Bucket Colorization, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2></strong>, 2024. <a href="https://arxiv.org/abs/2403.18342" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://ykdai.github.io/projects/InclusionMatching" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li> 
<li> LAMP: Learn A Motion Pattern for Few-Shot Video Generation, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2></strong>, 2024. <a href="https://arxiv.org/abs/2310.10769" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://rq-wu.github.io/projects/LAMP/index.html" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li> 
<li> Fourier Priors-Guided Diffusion for Zero-Shot Joint Low-Light Enhancement and Deblurring, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2></strong>, 2024. <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lv_Fourier_Priors-Guided_Diffusion_for_Zero-Shot_Joint_Low-Light_Enhancement_and_Deblurring_CVPR_2024_paper.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://github.com/aipixel/fourierdiff" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li> 
<li> Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond, <strong><ud2>IEEE Transactions on Pattern Analysis and Machine Intelligence</ud2></strong>, 2024. <a href="https://arxiv.org/abs/2306.04236" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://github.com/ykdai/Flare7K" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li> 
</ol>  
 
                                                                                                                                                                                                                                                                                                                                                  
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2023:</h4>

<ol>
<li> Iterative Prompt Learning for Unsupervised Backlit Image Enhancement, <strong><ud2>International Conference on Computer Vision (ICCV)</ud2></strong>, 2023. <a href="" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://github.com/ZhexinLiang/CLIP-LIT" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li> 
<li> ProPainter: Improving Video Inpainting with Enhanced Propagation and Efficient Transformer, <strong><ud2>International Conference on Computer Vision (ICCV)</ud2></strong>, 2023. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_ProPainter_Improving_Propagation_and_Transformer_for_Video_Inpainting_ICCV_2023_paper.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://github.com/sczhou/ProPainter" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising, <strong><ud2>International Conference on Computer Vision (ICCV)</ud2></strong>, 2023. <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jin_Lighting_Every_Darkness_in_Two_Pairs_A_Calibration-Free_Pipeline_for_ICCV_2023_paper.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="about:blank" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Fourmer: An Efficient Global Modeling Paradigm for Image Restoration, <strong><ud2>International Conference on Machine Learning (ICML)</ud2></strong>, 2023. <a href="https://proceedings.mlr.press/v202/zhou23f.html" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://github.com/manman1995/Fourmer" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li>  
<li> Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2></strong>, 2023. <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_Nighttime_Smartphone_Reflective_Flare_Removal_Using_Optical_Center_Symmetry_Prior_CVPR_2023_paper.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://ykdai.github.io/projects/BracketFlare" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li>  
<li> DNF: Decouple and Feedback Network for Seeing in the Dark, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2></strong>, 2023. <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_DNF_Decouple_and_Feedback_Network_for_Seeing_in_the_Dark_CVPR_2023_paper.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://github.com/Srameo/DNF" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li>  
<li> RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2></strong>, 2023. <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_RIDCP_Revitalizing_Real_Image_Dehazing_via_High-Quality_Codebook_Priors_CVPR_2023_paper.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://rq-wu.github.io/projects/RIDCP/index.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li>  
<li> Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2></strong>, 2023. <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Learning_Semantic-Aware_Knowledge_Guidance_for_Low-Light_Image_Enhancement_CVPR_2023_paper.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://github.com/langmanbusi/Semantic-Aware-Low-Light-Image-Enhancement" target="_blank"><font color="#1C86EE">[Project]</font></a></li>  
<li> Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2></strong>, 2023. <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Generating_Aligned_Pseudo-Supervision_From_Non-Aligned_Data_for_Image_Restoration_in_CVPR_2023_paper.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://github.com/jnjaby/AlignFormer" target="_blank"><font color="#1C86EE">[Project]</font></a></li>  
<li> Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement, <strong><ud2>International Conference on Learning Representations (ICLR)</ud2></strong>, 2023. <a href="https://arxiv.org/pdf/2302.11831" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://li-chongyi.github.io/UHDFour/" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li>  
<li> Underwater Ranker: Learn Which Is Better and How to Be Better, <strong><ud2>AAAI Conference on Artificial Intelligence (AAAI)</ud2></strong>, 2023. <a href="https://arxiv.org/pdf/2208.06857.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a><a href="https://li-chongyi.github.io/URanker_files/" target="_blank"><font color="#1C86EE">[Project]</font></a></li>  
<li> Is Underwater Image Enhancement All Object Detectors Need?, <strong><ud2>IEEE Journal of Oceanic Engineering</ud2></strong> (SCI, IF=3.554), 2023. <a href="https://arxiv.org/pdf/2311.18814" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a><a href="https://li-chongyi.github.io/Underwater_Object_Detection_files/" target="_blank"><font color="#1C86EE">[Code]</font></a></li> 
</ol>  
</ol>  

<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2022:</h4>

<ol>
<li> Deep Fourier Up-Sampling, <strong><ud2>Neural Information Processing Systems (NeurIPS)</ud2></strong>, 2022. <a href="https://arxiv.org/abs/2210.05171" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://li-chongyi.github.io/FourierUp_files/" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000"></font></li> 
<li> Towards Robust Blind Face Restoration with Codebook Lookup TransFormer, <strong><ud2>Neural Information Processing Systems (NeurIPS)</ud2></strong>, 2022. <a href="https://arxiv.org/pdf/2206.11253.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://shangchenzhou.com/projects/CodeFormer/" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Flare7K: A Phenomenological Nighttime Flare Removal Dataset, <strong><ud2>Neural Information Processing Systems (NeurIPS)</ud2></strong>, 2022. <a href="https://arxiv.org/abs/2210.06570" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://nukaliad.github.io/projects/Flare7K" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> LEDNet: Joint Low-light Enhancement and Deblurring in the Dark, <strong><ud2>European Conference on Computer Vision (ECCV)</ud2></strong>, 2022. <a href="https://arxiv.org/pdf/2202.03373.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://github.com/sczhou/LEDNet" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://shangchenzhou.com/projects/LEDNet/" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Underwater Image Enhancement with Hyper-Laplacian Reflectance Priors, <strong><ud2>IEEE Transactions on Image Processing</ud2></strong> (SCI, IF=10.856), 2022. <a href="https://ieeexplore.ieee.org/document/9854113" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="https://github.com/zhuangpeixian/HLRP" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://github.com/zhuangpeixian/HLRP" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000">[ESI Hot Paper & Highly Cited Paper][TIP Popular Documents]</font></li> 
<li> Underwater Image Enhancement via Minimal Color Loss and Locally Adaptive Contrast Enhancement, <strong><ud2>IEEE Transactions on Image Processing</ud2></strong> (SCI, IF=10.856), 2022. <a href="https://ieeexplore.ieee.org/document/9788535" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="https://github.com/Li-Chongyi/MMLE_code" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://li-chongyi.github.io/proj_MMLE" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000">[ESI Hot Paper & Highly Cited Paper][TIP Popular Documents]</font></li> 
<li> CIR-Net: Cross-modality Interaction and Refinement for RGB-D Salient Object Detection, <strong><ud2>IEEE Transactions on Image Processing</ud2></strong> (SCI, IF=10.856), 2022. <a href=" " target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="https://github.com/rmcong/CIRNet_TIP2022" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://rmcong.github.io/proj_CIRNet.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Global-and-Local Collaborative Learning for Co-Salient Object Detection, <strong><ud2>IEEE Transactions on Cybernetics</ud2></strong> (SCI, IF=11.448), 2022. <a href="https://arxiv.org/abs/2204.08917" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="https://github.com/rmcong/GLNet_TCYB2022" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://rmcong.github.io/proj_GLNet.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> A Feature Selection Algorithm Based on Equal Interval Division and Conditional Mutual Information, <strong><ud2>Neural Processing Letters</ud2></strong> (SCI, IF=2.908), 2022.  <a href="PDF/A_Feature_Selection_Algorithm_Based_on_Equal_Interval_Division_and_Conditional_Mutual_Information.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>

</ol> 
  
<h4>
 
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2021:</h4>

<ol>
<li> Low-Light Image and Video Enhancement Using Deep Learning: A Survey, <strong><ud2>IEEE Transactions on Pattern Analysis and Machine Intelligence </ud2></strong>(SCI, IF=17.861), 2021.  <a href="https://arxiv.org/abs/2104.10729" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://ieeexplore.ieee.org/document/9609683" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="PDF/Low_Light_Image_and_Video_Enhancement_Using_Deep_Learning__A_Survey.pdf"><font color="#1C86EE">[ä¸­æ–‡ç‰ˆ ç¿»è¯‘ by Xuanyu Li]</font></a> <a href="https://www.mmlab-ntu.com/project/lliv_survey/index.html" target="_blank"><font color="#1C86EE">[Project]</font></a> <a href="http://mc.nankai.edu.cn/ll/" target="_blank"><font color="#1C86EE">[Online Platform]</font></a> <a href="https://drive.google.com/file/d/1QS4FgT5aTQNYy-eHZ_A89rLoZgx_iysR/view" target="_blank"><font color="#1C86EE">[Dataset]</font></a> <a href="https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open//" target="_blank"><font color="#1C86EE">[Collections]</font></a><font color="#ff0000">[ESI Highly Cited Paper & ESI Hot Paper]</font></li>  
<li> Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation, <strong><ud2>IEEE Transactions on Pattern Analysis and Machine Intelligence</ud2></strong> (SCI, IF=17.861), 2021.  <a href="https://arxiv.org/abs/2103.00860" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://ieeexplore.ieee.org/document/9369102" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/Proj_Zero-DCE++.html" target="_blank"><font color="#1C86EE">[Project]</font></a><a href="https://www.bilibili.com/video/BV1Gv411j7HN?p=2" target="_blank"><font color="#1C86EE">[Invited Talk å•†æ±¤å­¦æœ¯ AIç”»è´¨@Bç«™, in Chinese]</font></a>-Our Zero-DCE (Zero-DCE++) for low-light image enhancement was used in the 1st place solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition.<a href="https://cvpr2021.ug2challenge.org/#" target="_blank"><font color="#1C86EE">[UG2+ Challenge]</font></a><a href="https://arxiv.org/abs/2107.00818" target="_blank"><font color="#1C86EE">[1st place solution for face detection in the low light condition]</font></a><font color="#ff0000">[ESI Highly Cited Paper][TPAMI Popular Documents]</font></li>   
<li> Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </ud2></strong>, 2021. <a href="https://arxiv.org/abs/2104.09556" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Feng_Removing_Diffraction_Image_Artifacts_in_Under-Display_Camera_via_Dynamic_Skip_CVPR_2021_paper.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://github.com/jnjaby/DISCNet" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://jnjaby.github.io/projects/UDC/" target="_blank"><font color="#1C86EE">[Project]</font></a><a href="https://drive.google.com/file/d/1QS4FgT5aTQNYy-eHZ_A89rLoZgx_iysR/view" target="_blank"><font color="#1C86EE">[Dataset]</font></a> <a href="https://www.rsipvision.com/ComputerVisionNews-2021October/Computer%20Vision%20News.pdf" target="_blank"><font color="#1C86EE">[Covered by Computer Vision News]</font></a></li> 
<li> Investigating Attention Mechanism in 3D Point Cloud Object Detection,  <strong><ud2>International Conference on 3D Vision 2021 (3DV) </ud2></strong>, 2021. <a href="https://arxiv.org/abs/2108.00620" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a><a href="https://arxiv.org/abs/2108.00620" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a><a href="https://github.com/ShiQiu0419/attentions_in_3D_detection" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Underwater Image Enhancement by Attenuated Color Channel Correction and Detail Preserved Contrast Enhancement, <strong><ud2>IEEE Journal of Oceanic Engineering</ud2></strong> (SCI, IF=3.554), 2021. <a href="https://ieeexplore.ieee.org/abstract/document/9744022" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a><a href="https://github.com/Li-Chongyi/JOE2021_ACDC" target="_blank"><font color="#1C86EE">[Code]</font></a><font color="#ff0000">[ESI Highly Cited Paper]</font></li> 
<li> Bayesian Retinex Underwater Image Enhancement, <strong><ud2>Engineering Applications of Artificial Intelligence</ud2></strong> (SCI, IF=3.526), 2021. <a href="https://www.sciencedirect.com/science/article/abs/pii/S095219762100018X" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a><font color="red">[ESI Highly Cited Paper]</font></li>  
<li> Conditional Mutual Information-Based Feature Selection Algorithm for Maximal Relevance Minimal Redundancy, <strong><ud2>Applied Intelligence</ud2></strong> (SCI, IF=3.325), 2021. <a href="https://link.springer.com/article/10.1007/s10489-021-02412-4" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>


</ol> 
  
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2020:</h4>

<ol>
<li> RGB-D Salient Object Detection with Cross-Modality Modulation and Selection, <strong><ud2>European Conference on Computer Vision (ECCV) </ud2></strong>, 2020. <a href="https://arxiv.org/abs/2007.07051" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/Proj_ECCV20" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </ud2></strong>, 2020. <a href="https://arxiv.org/abs/2001.06826" target="_blank"><font color="#1C86EE">[PDF-Arxiv version]</font></a> <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html" target="_blank"><font color="#1C86EE">[Project]</font></a><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Zero-Reference%20Deep%20Curve%20Estimation%20for%20Low-Light%20Image%20Enhancement--CVPR2020%E4%B8%AD%E5%9B%BD%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E4%BC%9A.pptx" target="_blank"><font color="#1C86EE">[å¾®è½¯äºšç ”CVPR2020ä¸­å›½è®ºæ–‡åˆ†äº«ä¼š]</font></a></li> 
<li> NuI-Go: Recursive Non-Local Encoder-Dcoder Network for Retinal Image Non-Uniform Illumination Removal, <strong><ud2>ACM Multimedia (ACM MM) </ud2></strong>, 2020. <a href="https://arxiv.org/abs/2008.02984" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://dl.acm.org/doi/10.1145/3394171.3413928" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/Proj_ACMMM20_NuI-Go" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Diving Deeper Into Underwater Image Enhancement: A Survey, <strong><ud2>Signal Processing: Image Communication</ud2></strong> (SCI, IF=2.814), 2020. <a href="https://arxiv.org/pdf/1907.07863.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0923596520301478" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>  
<li> Stereo Superpixel: An Iterative Framework Based on Parallax Consistency and Collaborative Optimization, <strong><ud2>Information Science</ud2></strong> (SCI, IF=5.524), 2020. <a href="https://www.sciencedirect.com/science/article/abs/pii/S002002552031197X" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>  
</ol>    
  
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2019:</h4>

<ol>
<li> Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement, <strong><ud2>Pattern Recognition</ud2></strong> (SCI, IF=5.898), 2019. <a href="https://www.sciencedirect.com/science/article/pii/S0031320319303401" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://arxiv.org/abs/1807.03528" target="_blank"><font color="#1C86EE">[PDF--Arxiv version]</font></a> <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="red">[ESI Highly Cited Paper]</font><font color="red">[Pattern Recognition Best Paper Honourable Mention]</font></li>
<li> PDR-Net: Perception-Inspired Single Image Dehazing Network with Refinement, <strong><ud2>IEEE Transaction on Multimedia</ud2></strong> (SCI, IF=5.452), 2019. <a href="https://ieeexplore.ieee.org/document/8792133" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li> 
<li> A Feature Selection Algorithm Based on Equal Interval Division and Minimal-Redundancyâ€“Maximal-Relevance, <strong><ud2>Neural Processing Letters</ud2></strong> (SCI, IF=2.591), 2019.  <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/A%20visual%20hierarchical%20framework%20based%20model%20for%20underwater.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
  

</ol>  
  
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a> Before 2018:</h4>

<ol>

<li> Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution, <strong><ud2>IEEE Transaction on Image Processing</ud2></strong> (SCI, IF=9.34), 2018. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8579111" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/proj_SR.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li>
<li> LightenNet: A Convolutional Neural Network for Weakly Illuminated Image Enhancement,  <strong><ud2>Pattern Recognition Letters (PRL)</ud2> </strong> (SCI, IF=1.995), 2018. <a href="http://www.porikli.com/mysite/pdfs/porikli%202018%20-%20LightenNet:%20a%20Convolutional%20Neural%20Network%20for%20weakly%20illuminated%20image%20enhancement.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/proj_lowlight.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Emerging From Water: Underwater Image Color Correction Based on Weakly Supervised Color Transfer, <strong><ud2>IEEE Signal Processing Letters (SPL)</ud2> </strong> (SCI, IF=2.528), 2018. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Emerging%20From%20Water_Underwater%20Image%20Color%20Correction%20Based%20on%20Weakly%20Supervised%20Color%20Transfer%20.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <font color="red">[ESI Highly Cited Paper]</font><a href="https://li-chongyi.github.io/proj_Emerging_water.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li>   
<li> A Hybrid Method for Underwater Image Correction,  <strong><ud2>Pattern Recognition Letters (PRL)</ud2> </strong> (SCI, IF=1.995), 2017. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/A%20hybrid%20method%20for%20underwater%20image%20correction.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
<li> Underwater Image Enhancement by Dehazing with Minimum Information Loss and Histogram Distribution Prior, <strong><ud2>IEEE Transactions on Image Processing (TIP)</ud2></strong> (SCI, IF=9.34), 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Underwater%20Image%20Enhancement%20by%20Dehazing%20With%20Minimum%20Information%20Loss%20and%20Histogram%20Distribution%20Prior.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://github.com/Li-Chongyi/TIP2016-code" target="_blank"><font color="#1C86EE">[Code]</font></a><font color="red">[ESI Highly Cited Paper]</font><a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=83" target="_blank"><font color="#ff0000">[TIP Popular Documents]</font></a></li>   
<li> Single Underwater Image Enhancement Based on Color Cast Removal and Visibility Restoration, <strong><ud2>Journal of Electronic Imaging (JEI)</ud2></strong> (SCI, IF=0.754), 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Single%20underwater%20image%20enhancement%20based%20on%20color%20cast%20removal%20and%20visibility%20restoration.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
<li> Underwater Image Restoration Based on Minimum Information Loss Principle and Optical Properties of Underwater Imaging, <strong><ud2>IEEE International Conference on Image Processing (ICIP)</ud2></strong>, 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Underwater%20image%20restoration%20based%20on%20minimum%20information%20loss%20principle%20and%20optical%20properties%20of%20underwater%20imaging.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
<li> Single Underwater Image Restoration by Blue-Green Channels Dehazing and Red Channel Correction, <strong><ud2>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</ud2></strong>, 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Single%20underwater%20image%20restoration%20by%20blue-green%20channels%20dehazing%20and%20red%20channel%20correction.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>

</ol>

 <h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Book&Book Chapter:</h4>

<ol>
<li> Deep Retinal Image Non-Uniform Illumination Removal (Book Chapter)<a href="https://www.worldscientific.com/doi/abs/10.1142/9789811218842_0010" target="_blank"><font color="#1C86EE">[Chapter]</font></a>, Generalization with Deep Learning: for improvement on Sensing Capability, Zhenghua Chen, Min Wu, and Xiaoli Li (Book), <strong><ud2>World Scientific</ud2></strong>, ISBN: 978-981-121-883-5, April 2021. <a href="https://www.worldscientific.com/worldscibooks/10.1142/11784" target="_blank"><font color="#1C86EE">[Book]</font></a></li>   
</ol> 




</section>

</div>
<script src="../javascripts/scale.fix.js"></script>
</body>
</html>
