<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<head>
 <link rel="Shortcut Icon" href="./logo/hp_logo.jpg" sizes=16x16  type="image/x-icon" />
 <link rel="Bookmark" href="./logo/hp_logo.jpg" sizes=16x16 type="image/x-icon" />
  
<title>Publications</title>

<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<meta name="viewport" content="width=device-width">
</head>
<body>
<div class="wrapper">
<header>
<h7>Chongyi Li</h7><br><br>
<div>
<img src="sub_img/lichongyi_photo_4.jpg" border="0" width="80%"><br></div><br>

  
<p>
<small>ğŸ“§lichongyi25 at gmail.com</small><br>
<small>ğŸ“§chongyi.li at ntu.edu.sg</small><br><br>
<a href="https://github.com/Li-Chongyi" target="_blank">[GitHub]</a>  
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chongyi" target="_blank">[DBLP]</a>  <br>
<a href="https://scholar.google.com/citations?user=1_I0P-AAAAAJ&hl=en" target="_blank">[Google Scholar]</a> <br>
</p> <br>
<p class="view"><a href="https://li-chongyi.github.io/">Homepage</a></p>
<p class="view"><a href="sub_publication.html">Publications</a></p>
<p class="view"><a href="datasets.html">Datasets</a></p>
</header>

<section>

<h2>
<a id="publications-pages" class="anchor" href="#publications-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications:</h2>
<div style="text-align: justify; display: block; margin-right: auto;">
<!--
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Under Review</h4>

<ol>

<li> Saeed Anwar <strong>Chongyi Li</strong>, Fatih Porikli, Deep underwater image enhancement, in <strong><ud2>arXiv: 1807.03528</ud2></strong>, 2018.  <a href="https://arxiv.org/pdf/1807.03528.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a> </li>
<li> <strong>Chongyi Li</strong>, Jichang Guo, Fatih porikli, et al., DR-Net: Transmission steered single image dehazing network with weakly supervised refinement,  in <strong><ud2>arXiv: 1712.00621</ud2> </strong>, 2017. <a href="https://arxiv.org/abs/1712.00621" target="_blank"><font color="#1C86EE">[PDF]</font></a> </li>  

<li> <strong>Chongyi Li</strong>, Saeed Anwar, Junhui Hou, et al., Underwater image enhancement via depth attention-guided multi-color space embedding, <strong><ud2> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</ud2></strong>  </li>   
<li> Wenqi Ren <strong>Chongyi Li</strong>, Lin Ma, et al., Multi-scale gated fusion network for single image dehazing and beyond, <strong><ud2> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</ud2></strong> (co-first author) </li>
<li> Chunle Guo, <strong>Chongyi Li</strong>, Jichang Guo, et al., Zero-reference deep curve estimation for low-light image enhancement, <strong><ud2> Top Conference</ud2></strong>  (co-first author) </li>   
<li> Runmin Cong, <strong>Chongyi Li</strong>, et al., Multi-Modality Soft-to-Hard Attention Network for RGB-D Saliency Detection, <strong><ud2> Top Conference</ud2></strong>  (co-first and corresponding author) </li>   
<li> <strong>Chongyi Li</strong>, Runmin Cong, Chunle Guo, et al., A Parallel Down-Up Fusion Network for Salient Object Detection in Optical Remote Sensing Images, <strong><ud2>Neurocomputing</ud2></strong>  </li>   
<li> Saeed Anwar and <strong>Chongyi Li</strong>, Diving Deeper into Underwater Image Enhancement: A Survey, <strong><ud2> IJCV</ud2></strong> <a href="https://arxiv.org/pdf/1907.07863.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a> (co-first author) </li>
<li> Miao Yang, Hu Ke, <strong>Chongyi Li</strong>, et al., UW-Net: An inception-attention network for underwater image classification, <strong><ud2> IEEE Transactions on Image Processing (TIP)</ud2></strong>  </li>   
<li> Hua Li, Runmin Cong, <strong>Chongyi Li</strong>, et al., Stereo Superpixel: An Iterative Framework based on Parallax Consistency and Collaborative Optimization, <strong><ud2> Top Conference</ud2></strong> </li>   

</ol>   
  --> 
 <h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Book&Book Chapter:</h4>

<ol>
<li> <strong>Chongyi Li</strong>, Huazhu Fu, Miao Yang, Runmin Cong, and Chunle Guo, Deep Retinal Image Non-Uniform Illumination Removal (Book Chapter)<a href="https://www.worldscientific.com/doi/abs/10.1142/9789811218842_0010" target="_blank"><font color="#1C86EE">[Chapter]</font></a>, Generalization with Deep Learning: for improvement on Sensing Capability, Zhenghua Chen, Min Wu, and Xiaoli Li (Book), <strong><ud2>World Scientific</ud2></strong>, ISBN: 978-981-121-883-5, April 2021. <a href="https://www.worldscientific.com/worldscibooks/10.1142/11784" target="_blank"><font color="#1C86EE">[Book]</font></a></li>   
</ol> 
  
 
 <h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preprint:</h4>

<ol>
<li> Chunle Guo, Ruiqi Wu, Xin Jin, Linghao Han, Zhi Chai, Weidong Zhang, and <strong>Chongyi Li<sup>+</sup></strong> <strong>Underwater Ranker: Learn Which Is Better and How to Be Better</strong>. <a href="https://arxiv.org/pdf/2208.06857.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a></li> 
<li> <strong>Chongyi Li</strong>, Chunle Guo, Ruicheng Feng, Shangchen Zhou, and Chen Change Loy, <strong>CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment</strong>. <a href="https://arxiv.org/pdf/2207.14273.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://li-chongyi.github.io/CuDi_files/" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Shangchen Zhou, Kelvin C. K. Chan, <strong>Chongyi Li</strong>, and Chen Change Loy, <strong>Towards Robust Blind Face Restoration with Codebook Lookup TransFormer</strong>. <a href="https://arxiv.org/pdf/2206.11253.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://shangchenzhou.com/projects/CodeFormer/" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Saeed Anwar, Muhammad Tahir, <strong>Chongyi Li</strong>, et al., <strong>Image Colorization: A Survey and Dataset</strong>. <a href="https://arxiv.org/pdf/2008.10774.pdf" target="_blank"><font color="#1C86EE">[PDF--arXiv Version]</font></a><a href="https://github.com/saeed-anwar/ColorSurvey" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> <strong>Chongyi Li</strong>, Chunle Guo, Qiming Ai, Shangchen Zhou, Chen Change Loy,  <strong>Flexible Piecewise Curves Estimation for Photo Enhancement</strong>. <a href="https://arxiv.org/pdf/2010.13412.pdf" target="_blank"><font color="#1C86EE">[PDF--arXiv Version]</font></a> </li>
</ol> 
  


 
 <h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2022:</h4>

<ol>
<li> Shangchen Zhou, <strong>Chongyi Li</strong>, and Chen Change Loy, LEDNet: Joint Low-light Enhancement and Deblurring in the Dark, <strong><ud2>European Conference on Computer Vision (ECCV)</ud2></strong>, 2022. <a href="https://arxiv.org/pdf/2202.03373.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://github.com/sczhou/LEDNet" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://shangchenzhou.com/projects/LEDNet/" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Chunle Guo,  Qixin Yan,  Saeed Anwar, Runmin Cong, Wenqi Ren, and <strong>Chongyi Li<sup>+</sup></strong>, Image Dehazing Transformer with Transmission-Aware 3D Position Embedding, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</ud2></strong>, 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="https://github.com/Li-Chongyi/Dehamer" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://li-chongyi.github.io/Proj_DeHamer.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Man Zhou, Jie Huang, <strong>Chongyi Li<sup>+</sup></strong>, Hu Yu, Keyu Yan, Naishan Zheng, and Feng Zhao, Adaptively Learning Low-high Frequency Information Integration for Pan-sharpening, <strong><ud2>ACM International Conference on Multimedia (ACM MM)</ud2></strong>, 2022. <a href="" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Man Zhou, Jie Huang, Keyu Yan, Gang Yang, Aiping Liu, <strong>Chongyi Li</strong>, and Feng Zhao, 	Normalization-based Feature Selection and Restitution for Pan-sharpening, <strong><ud2>ACM International Conference on Multimedia (ACM MM)</ud2></strong>, 2022. <a href="" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Peixian Zhuang, Jiamin Wu, Fatih Porikli, and <strong>Chongyi Li<sup>+</sup></strong>, Underwater Image Enhancement with Hyper-Laplacian Reflectance Priors, <strong><ud2>IEEE Transactions on Image Processing</ud2></strong> (SCI, IF=10.856), 2022. <a href="https://ieeexplore.ieee.org/document/9854113" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="https://github.com/zhuangpeixian/HLRP" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://github.com/zhuangpeixian/HLRP" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Weidong Zhang, Peixian Zhuang, Haihan Sun, Guohou Li, Sam Kwong, and <strong>Chongyi Li<sup>+</sup></strong>, Underwater Image Enhancement via Minimal Color Loss and Locally Adaptive Contrast Enhancement, <strong><ud2>IEEE Transactions on Image Processing</ud2></strong> (SCI, IF=10.856), 2022. <a href="https://ieeexplore.ieee.org/document/9788535" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="https://github.com/Li-Chongyi/MMLE_code" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://li-chongyi.github.io/proj_MMLE" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Runmin Cong, Ning Yang,  <strong>Chongyi Li<sup>+</sup></strong>, Huazhu Fu, Yao Zhao, Qingming Huang, and Sam Kwong, Global-and-Local Collaborative Learning for Co-Salient Object Detection, <strong><ud2>IEEE Transactions on Cybernetics</ud2></strong> (SCI, IF=11.448), 2022. <a href="https://arxiv.org/abs/2204.08917" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="https://github.com/rmcong/GLNet_TCYB2022" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://rmcong.github.io/proj_GLNet.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Jun Luo,  Wenqi Ren, Tao Wang, <strong>Chongyi Li</strong>, and Xiaochun Cao, Under-Display Camera Image Enhancement via Cascaded Curve Estimation, <strong><ud2>IEEE Transactions on Image Processing</ud2></strong> (SCI, IF=10.856), 2022. <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/document/9798712" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a></li> 
<li> Yaozu Kang, Qiuping Jiang, <strong>Chongyi Li</strong>, Hantao Liu, and Pengjun Wang, A Perception-Aware Decomposition and Fusion Framework for Underwater Image Enhancement, <strong><ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2></strong> (SCI, IF=4.685), 2022. <a href="" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a></li> 
<li> Qiuping Jiang,  Yuese Gu,  <strong>Chongyi Li</strong>, Runming Cong, and Feng Shao, Underwater Image Enhancement Quality Evaluation: Dataset and Metric, <strong><ud2>IEEE Transactions on Circuits and Systems for Video Technology</ud2></strong> (SCI, IF=4.685), 2022. <a href="https://ieeexplore.ieee.org/document/9749233" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></font></a><a href="https://github.com/yia-yuese/SAUD-Dataset" target="_blank"><font color="#1C86EE">[Code&Dataset]</font></a><a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=76" target="_blank"><font color="#ff0000">[TCSVT Popular Documents]</font></a></li></li> 
<li> Xiangyuan Gu, Jichang Guo,  Tao Ming, Lijun Xiao, and <strong>Chongyi Li</strong>, A Feature Selection Algorithm Based on Equal Interval Division and Conditional Mutual Information, <strong><ud2>Neural Processing Letters</ud2></strong> (SCI, IF=2.908), 2022.  <a href="PDF/A_Feature_Selection_Algorithm_Based_on_Equal_Interval_Division_and_Conditional_Mutual_Information.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>

</ol> 
  
<h4>
 
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2021:</h4>

<ol>
<li> <strong>Chongyi Li</strong>, Chunle Guo, Linghao Han, et al., Low-Light Image and Video Enhancement Using Deep Learning: A Survey, <strong><ud2>IEEE Transactions on Pattern Analysis and Machine Intelligence </ud2></strong>(SCI, IF=17.861), 2021.  <a href="https://arxiv.org/abs/2104.10729" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://ieeexplore.ieee.org/document/9609683" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="PDF/Low_Light_Image_and_Video_Enhancement_Using_Deep_Learning__A_Survey.pdf"><font color="#1C86EE">[ä¸­æ–‡ç‰ˆ ç¿»è¯‘ by Xuanyu Li]</font></a> <a href="https://www.mmlab-ntu.com/project/lliv_survey/index.html" target="_blank"><font color="#1C86EE">[Project]</font></a> <a href="http://mc.nankai.edu.cn/ll/" target="_blank"><font color="#1C86EE">[Online Platform]</font></a> <a href="https://drive.google.com/file/d/1QS4FgT5aTQNYy-eHZ_A89rLoZgx_iysR/view" target="_blank"><font color="#1C86EE">[Dataset]</font></a> <a href="https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open//" target="_blank"><font color="#1C86EE">[Collections]</font></a></li>   
<li> <strong>Chongyi Li</strong>, Chunle Guo, and Chen Change Loy, Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation, <strong><ud2>IEEE Transactions on Pattern Analysis and Machine Intelligence</ud2></strong> (SCI, IF=17.861), 2021.  <a href="https://arxiv.org/abs/2103.00860" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://ieeexplore.ieee.org/document/9369102" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/Proj_Zero-DCE++.html" target="_blank"><font color="#1C86EE">[Project]</font></a><a href="https://www.bilibili.com/video/BV1Gv411j7HN?p=2" target="_blank"><font color="#1C86EE">[Invited Talk å•†æ±¤å­¦æœ¯ AIç”»è´¨@Bç«™, in Chinese]</font></a>-Our Zero-DCE (Zero-DCE++) for low-light image enhancement was used in the 1st place solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition.<a href="https://cvpr2021.ug2challenge.org/#" target="_blank"><font color="#1C86EE">[UG2+ Challenge]</font></a><a href="https://arxiv.org/abs/2107.00818" target="_blank"><font color="#1C86EE">[1st place solution for face detection in the low light condition]</font></a><font color="#ff0000">[TPAMI Popular Documents]</font></li>   
<li> <strong>Chongyi Li</strong>, Saeed Anwar, Junhui Hou, et al., Underwater Image Enhancement via Medium Transmission-Guided Multi-Color Space Embedding, <strong><ud2>IEEE Transactions on Image Processing</ud2></strong> (SCI, IF=9.34), 2021. <a href="https://ieeexplore.ieee.org/document/9426457" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a><a href="https://arxiv.org/abs/2104.13015" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://li-chongyi.github.io/Proj_Ucolor.html" target="_blank"><font color="#1C86EE">[Project]</font></a><a href="https://github.com/Li-Chongyi/Ucolor" target="_blank"><font color="#1C86EE">[Code]</font></a></li>
<li> Ruicheng Feng, <strong>Chongyi Li</strong>, Huaijin Chen, Shuai Li, Chen Change Loy, Jinwei Gu, Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </ud2></strong>, 2021. <a href="https://arxiv.org/abs/2104.09556" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Feng_Removing_Diffraction_Image_Artifacts_in_Under-Display_Camera_via_Dynamic_Skip_CVPR_2021_paper.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://github.com/jnjaby/DISCNet" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://jnjaby.github.io/projects/UDC/" target="_blank"><font color="#1C86EE">[Project]</font></a><a href="https://drive.google.com/file/d/1QS4FgT5aTQNYy-eHZ_A89rLoZgx_iysR/view" target="_blank"><font color="#1C86EE">[Dataset]</font></a> <a href="https://www.rsipvision.com/ComputerVisionNews-2021October/Computer%20Vision%20News.pdf" target="_blank"><font color="#1C86EE">[Covered by Compputer Vision News]</font></a></li> 
<li> Shi Qiu, Yunfan Wu, Saeed Anwar, and <strong>Chongyi Li</strong>, Investigating Attention Mechanism in 3D Point Cloud Object Detection,  <strong><ud2>International Conference on 3D Vision 2021 (3DV) </ud2></strong>, 2021. <a href="https://arxiv.org/abs/2108.00620" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a><a href="https://arxiv.org/abs/2108.00620" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a><a href="https://github.com/ShiQiu0419/attentions_in_3D_detection" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Weidong Zhang,  <strong>Chongyi Li</strong>, and Yudong Wang, Underwater Image Enhancement by Attenuated Color Channel Correction and Detail Preserved Contrast Enhancement, <strong><ud2>IEEE Journal of Oceanic Engineering</ud2></strong> (SCI, IF=3.554), 2021. <a href="https://ieeexplore.ieee.org/abstract/document/9744022" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a><a href="https://github.com/Li-Chongyi/JOE2021_ACDC" target="_blank"><font color="#1C86EE">[Code]</font></a></li> 
<li> Peixian Zhuang, <strong>Chongyi Li</strong>, and Jiamin Wu, Bayesian Retinex Underwater Image Enhancement, <strong><ud2>Engineering Applications of Artificial Intelligence</ud2></strong> (SCI, IF=3.526), 2021. <a href="https://www.sciencedirect.com/science/article/abs/pii/S095219762100018X" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li> 
<li> Hai-Han Sun, Yee Hui Lee, Qiqi Dai, and <strong>Chongyi Li</strong>, et al., Estimating Parameters of the Tree Root in Heterogeneous Soil Environments via Mask-Guided Multi-Polarimetric Integration Neural Network, <strong><ud2>IEEE Transactions  Geoscience and Remote Sensing</ud2></strong> (SCI, IF=5.6), 2021. <a href="" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a></li>   
<!--<li> Hai-Han Sun, Yee Hui Lee, and <strong>Chongyi Li</strong>, et al., The Orientation Estimation of Elongated Underground Objects via Multi-Polarization Aggregation and Selection Neural Network, <strong><ud2>IEEE Geoscience and Remote Sensing Letters</ud2></strong> (SCI, IF=3.833), 2021. <a href="https://arxiv.org/ftp/arxiv/papers/2101/2101.12398.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a></li>--> 
<li> Xiangyuan Gu, Jichang Guo, Lijun Xiao, and <strong>Chongyi Li</strong>, Conditional Mutual Information-Based Feature Selection Algorithm for Maximal Relevance Minimal Redundancy, <strong><ud2>Applied Intelligence</ud2></strong> (SCI, IF=3.325), 2021. <a href="https://link.springer.com/article/10.1007/s10489-021-02412-4" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>
<li> Jingchun Zhou, Yanyun Wang, Weishi Zhang, and <strong>Chongyi Li</strong>, Underwater Image Restoration via Feature Priors to Estimate Background Light and Optimized Transmission Map, <strong><ud2>Optics Express</ud2></strong> (SCI, IF=3.669), 2021. <a href="https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-29-18-28228&id=457313" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li> 

</ol> 
  
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2020:</h4>

<ol>
<li> <strong>Chongyi Li</strong>, Runmin Cong, Yongri Piao, et al., RGB-D Salient Object Detection with Cross-Modality Modulation and Selection, <strong><ud2>European Conference on Computer Vision (ECCV) </ud2></strong>, 2020. <a href="https://arxiv.org/abs/2007.07051" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/Proj_ECCV20" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Chunle Guo, <strong>Chongyi Li (co-first author)</strong>, Jichang Guo, et al., Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement, <strong><ud2>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </ud2></strong>, 2020. <a href="https://arxiv.org/abs/2001.06826" target="_blank"><font color="#1C86EE">[PDF-Arxiv version]</font></a> <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html" target="_blank"><font color="#1C86EE">[Project]</font></a><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Zero-Reference%20Deep%20Curve%20Estimation%20for%20Low-Light%20Image%20Enhancement--CVPR2020%E4%B8%AD%E5%9B%BD%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E4%BC%9A.pptx" target="_blank"><font color="#1C86EE">[å¾®è½¯äºšç ”CVPR2020ä¸­å›½è®ºæ–‡åˆ†äº«ä¼š]</font></a></li> 
<li> <strong>Chongyi Li</strong>, Huazhu Fu, Runmin Cong, et al., NuI-Go: Recursive Non-Local Encoder-Dcoder Network for Retinal Image Non-Uniform Illumination Removal, <strong><ud2>ACM Multimedia (ACM MM) </ud2></strong>, 2020. <a href="https://arxiv.org/abs/2008.02984" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://dl.acm.org/doi/10.1145/3394171.3413928" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/Proj_ACMMM20_NuI-Go" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> Qijian Zhang, Runmin Cong, Junhui Hou, <strong>Chongyi Li</strong>, and Yao Zhao, CoADNet: Collaborative Aggregation-and-Distribution Networks for Co-Salient Object Detection,<strong><ud2>Neural Information Processing Systems (NeurIPS) </ud2></strong>, 2020. <a href="https://proceedings.neurips.cc/paper/2020/file/4dc3ed26a29c9c3df3ec373524377a5b-Paper.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://github.com/rmcong/CoADNet_NeurIPS20" target="_blank"><font color="#1C86EE">[Code]</font></a><a href="https://www.bilibili.com/video/BV1np4y1z7B7" target="_blank"><font color="#1C86EE">[æœºå™¨ä¹‹å¿ƒNeurIPS20Â MeetUp@åŒ—äº¬, in Chinese]</font></a></li> 
<li> <strong>Chongyi Li</strong>, Runmin Cong, Sam Kwong, et al., ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D Salient Object Detection, <strong><ud2>IEEE Transactions on Cybernetics </ud2></strong> (SCI, IF=10.387), 2020. <a href="https://ieeexplore.ieee.org/document/8998588" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://github.com/Li-Chongyi/ASIF-Net" target="_blank"><font color="#1C86EE">[Results+Codes]</font></a></font></a><font color="red">[ESI Highly Cited Paper]</font><a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=6221036" target="_blank"><font color="#ff0000">[TCYB Popular Documents]</font></a></li>
<li> <strong>Chongyi Li</strong>, Runmin Cong, Chunle Guo, et al., A Parallel Down-Up Fusion Network for Salient Object Detection in Optical Remote Sensing Images, <strong><ud2>Neurocomputing</ud2></strong> (SCI, IF=4.072), 2020. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220313692" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>
<li> Saeed Anwar, <strong>Chongyi Li<sup>+</sup></strong>, Diving Deeper Into Underwater Image Enhancement: A Survey, <strong><ud2>Signal Processing: Image Communication</ud2></strong> (SCI, IF=2.814), 2020. <a href="https://arxiv.org/pdf/1907.07863.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0923596520301478" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>  
<li> Qijian Zhang, Runmin Cong, <strong>Chongyi Li</strong>, et al., Dense Attention Fluid Network for Salient Object Detection in Optical Remote Sensing Images, <strong><ud2>IEEE Transactions on Image Processing</ud2></strong> (SCI, IF=9.34), 2020. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292434" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a><a href="https://arxiv.org/pdf/2011.13144.pdf" target="_blank"><font color="#1C86EE">[PDF-arXiv version]</font></a> <a href="https://rmcong.github.io/proj_DAFNet.html" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000">[ESI Highly Cited Paper & ESI Hot Paper]</font></li>
<!--<li> Qing Qi, Jichang Guo, <strong>Chongyi Li</strong>, and Lijun Xiao, Blind Face Images Deblurring with Enhancement, <strong><ud2>Multimedia Tools and Applications</ud2></strong> (SCI, IF=2.6), 2019.  <a href="https://link.springer.com/article/10.1007/s11042-020-09460-x" target="_blank"><font color="#1C86EE">[PDF-official version]</font></a> </li>--> 
<li> Xiangyuan Gu, Jichang Guo, <strong>Chongyi Li</strong>, Lijun Xiao, A Feature Selection Algorithm Based on Redundancy Analysis and Interaction Weight, <strong><ud2>Applied Intelligence</ud2></strong> (SCI, IF=2.882), 2020. <a href="https://link.springer.com/article/10.1007/s10489-020-01936-5" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>
<!--<li> Bo Wang, Bowen Wei, Zitong Kang, Li Hu, and <strong>Chongyi Li</strong>, Fast Color Balance and Multi-Path Fusion for Sandstorm Image Enhancement, <strong><ud2>Signal, Image and Video Processing</ud2></strong> (SCI, IF=1.794), 2020. <a href="https://link.springer.com/article/10.1007/s11760-020-01786-1" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>-->
<!--<li> Bo Wang, Li Hu, Bowen Wei, Zitong Kang, and <strong>Chongyi Li</strong>, Nighttime Image Dehazing Using Color Cast Removal and Dual Path Multi-Scale Fusion Strategy, <strong><ud2>Frontiers of Computer Science</ud2></strong> (SCI, IF=1.940), 2020. <a href="https://link.springer.com/article/10.1007/s11704-021-0162-x" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>--> 
<li> Hua Li, Runming Cong, Sam Kwong, Chuanbo Chen, Qianqian Xu, and <strong>Chongyi Li</strong>,  Stereo Superpixel: An Iterative Framework Based on Parallax Consistency and Collaborative Optimization, <strong><ud2>Information Science</ud2></strong> (SCI, IF=5.524), 2020. <a href="https://www.sciencedirect.com/science/article/abs/pii/S002002552031197X" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a></li>  
</ol>    
  
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>2019:</h4>

<ol>
<li> <strong>Chongyi Li</strong>,  Chunle Guo, Wenqi Ren, et al., An Underwater Image Enhancement Benchmark Dataset and Beyond, <strong><ud2>IEEE Transaction on Image Processing</ud2></strong> (SCI, IF=9.34), 2019. <a href="https://arxiv.org/pdf/1901.05495.pdf" target="_blank"><font color="#1C86EE">[PDF--arXiv version]</font></a> <a href="https://ieeexplore.ieee.org/document/8917818" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/proj_benchmark.html" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="red">[ESI Highly Cited Paper & ESI Hot Paper]</font><a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=83" target="_blank"><font color="#ff0000">[TIP Popular Documents]</font></a></li>   
<li> <strong>Chongyi Li</strong>, Saeed Anwar, and Fatih Porikli, Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement, <strong><ud2>Pattern Recognition</ud2></strong> (SCI, IF=5.898), 2019. <a href="https://www.sciencedirect.com/science/article/pii/S0031320319303401" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://arxiv.org/abs/1807.03528" target="_blank"><font color="#1C86EE">[PDF--Arxiv version]</font></a> <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="red">[ESI Highly Cited Paper]</font><font color="red">[Pattern Recognition Best Paper Honourable Mention]</font></li>
<li> <strong>Chongyi Li</strong>, Chunle Guo, Jichang Guo, et al., PDR-Net: Perception-Inspired Single Image Dehazing Network with Refinement, <strong><ud2>IEEE Transaction on Multimedia</ud2></strong> (SCI, IF=5.452), 2019. <a href="https://ieeexplore.ieee.org/document/8792133" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
<li> <strong>Chongyi Li</strong>, Runmin Cong, Junhui Hou, et al., Nested Network with Two-Stream Pyramid for Salient Object Detection in Optical Remote Sensing Images, <strong><ud2>IEEE Transaction on Geoscience and Remote Sensing</ud2></strong> (SCI, IF=4.662), 2019. <a href="https://ieeexplore.ieee.org/document/8793227" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/proj_optical_saliency.html" target="_blank"><font color="#1C86EE">[Project]</font></a><font color="#ff0000">[ESI Highly Cited Paper]</font></li>
<!--<li> Miao Yang, Jintong Hu, <strong>Chongyi Li</strong>, et al., An In-depth Survey of Underwater Image Enhancement and Restoration, <strong><ud2>IEEE ACCESS</ud2></strong> (SCI,IF.3.244), 2019. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8786104" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>-->
<!--<li> Bo Wang and <strong>Chongyi Li</strong>, A Visual Hierarchical Framework Based Model for Underwater Image Enhancement, <strong><ud2>Frontiers of Computer Science</ud2></strong> (SCI, IF=1.105), 2019.  <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/A%20visual%20hierarchical%20framework%20based%20model%20for%20underwater.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>-->  
<li> Xiangyuan Gu, Jichang Guo, Lijun Xiao, Tao Ming, and <strong>Chongyi Li</strong>, A Feature Selection Algorithm Based on Equal Interval Division and Minimal-Redundancyâ€“Maximal-Relevance, <strong><ud2>Neural Processing Letters</ud2></strong> (SCI, IF=2.591), 2019.  <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/A%20visual%20hierarchical%20framework%20based%20model%20for%20underwater.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
  

</ol>  
  
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a> Before 2018:</h4>

<ol>

<li> Chunle Guo, <strong>Chongyi Li<sup>+</sup></strong>, Jichang Guo, et al., Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution, <strong><ud2>IEEE Transaction on Image Processing</ud2></strong> (SCI, IF=9.34), 2018. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8579111" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/proj_SR.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li>
<!--<li> <strong>Chongyi Li</strong>, Jichang Guo, Fatih Porikli, et al., A Convolutional Neural Network Cascade for Single Image Dehazing, <strong><ud2>IEEE Access</ud2></strong> (SCI, IF=3.244), 2018.  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8323372" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>-->
<li> <strong>Chongyi Li</strong>, Jichang Guo, Fatih Porikli, et al., LightenNet: A Convolutional Neural Network for Weakly Illuminated Image Enhancement,  <strong><ud2>Pattern Recognition Letters (PRL)</ud2> </strong> (SCI, IF=1.995), 2018. <a href="http://www.porikli.com/mysite/pdfs/porikli%202018%20-%20LightenNet:%20a%20Convolutional%20Neural%20Network%20for%20weakly%20illuminated%20image%20enhancement.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://li-chongyi.github.io/proj_lowlight.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li> 
<li> <strong>Chongyi Li</strong>, Jichang Guo, Chunle Guo, Emerging From Water: Underwater Image Color Correction Based on Weakly Supervised Color Transfer, <strong><ud2>IEEE Signal Processing Letters (SPL)</ud2> </strong> (SCI, IF=2.528), 2018. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Emerging%20From%20Water_Underwater%20Image%20Color%20Correction%20Based%20on%20Weakly%20Supervised%20Color%20Transfer%20.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <font color="red">[ESI Highly Cited Paper]</font><a href="https://li-chongyi.github.io/proj_Emerging_water.html" target="_blank"><font color="#1C86EE">[Project]</font></a></li>   
<li> <strong>Chongyi Li</strong>, Jichang Guo, Chunle Guo, et al., A Hybrid Method for Underwater Image Correction,  <strong><ud2>Pattern Recognition Letters (PRL)</ud2> </strong> (SCI, IF=1.995), 2017. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/A%20hybrid%20method%20for%20underwater%20image%20correction.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
<!--<li>Yan Zhang, Jichang Guo, <strong>Chongyi Li</strong>, Image Compressed Sensing Based on Non-Convex Low-Rank Approximation,  <strong><ud2>Multimedia Tools and Applications (MTA)</ud2></strong> (SCI, IF=1.530), 2017. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Image%20Compressed%20sensing%20based%20on%20non-convex%20low-rank%20approximation.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>-->
<!--<li>Yan Zhang, Jichang Guo, <strong>Chongyi Li</strong>, Compressive Sensing in Wireless Multimedia Sensor Networks Based on Low-Rank Approximation,  <strong><ud2>IEEE International Conference on Communications (ICC)<ud2></strong>, 2017. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Compressive%20sensing%20in%20wireless%20multimedia%20sensor%20networks%20based%20on%20low-rank%20approximation.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>-->
<li><strong>Chongyi Li</strong>, Jichang Guo, Runmin Cong, et al., Underwater Image Enhancement by Dehazing with Minimum Information Loss and Histogram Distribution Prior, <strong><ud2>IEEE Transactions on Image Processing (TIP)</ud2></strong> (SCI, IF=9.34), 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Underwater%20Image%20Enhancement%20by%20Dehazing%20With%20Minimum%20Information%20Loss%20and%20Histogram%20Distribution%20Prior.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> <a href="https://github.com/Li-Chongyi/TIP2016-code" target="_blank"><font color="#1C86EE">[Code]</font></a><font color="red">[ESI Highly Cited Paper]</font><a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=83" target="_blank"><font color="#ff0000">[TIP Popular Documents]</font></a></li>   
<li><strong>Chongyi Li</strong>, Jichang Guo, Bo Wang, et al., Single Underwater Image Enhancement Based on Color Cast Removal and Visibility Restoration, <strong><ud2>Journal of Electronic Imaging (JEI)</ud2></strong> (SCI, IF=0.754), 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Single%20underwater%20image%20enhancement%20based%20on%20color%20cast%20removal%20and%20visibility%20restoration.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
<li><strong>Chongyi Li</strong>, Jichang Guo, Shanji Chen, et al., Underwater Image Resstoration Based on Minimum Information Loss Principle and Optical Properties of Underwater Imaging, <strong><ud2>IEEE International Conference on Image Processing (ICIP)</ud2></strong>, 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Underwater%20image%20restoration%20based%20on%20minimum%20information%20loss%20principle%20and%20optical%20properties%20of%20underwater%20imaging.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
<li><strong>Chongyi Li</strong>, Jichang Guo, Yanwei Pang, et al., Single Underwater Image Restoration by Blue-Green Channles Dehazing and Red Channel Correction, <strong><ud2>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</ud2></strong>, 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Single%20underwater%20image%20restoration%20by%20blue-green%20channels%20dehazing%20and%20red%20channel%20correction.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
<!--<li>Runmin Cong, Ping Han, <strong>Chongyi Li</strong>, et al, Manmade Target Extraction Based on Multi-Stage Decision and Its Application for Change Detection in Polarimetric Synthetic Aperture Radar Image, <strong><ud2>Journal of Electronic Imaging (JEI)</ud2></strong> (SCI, IF=0.754), 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Manmadetargetextractionbasedonmultistagedecisionanditsapplicationforchangedetectioninpolarimetricsyntheticapertureradarimage.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>--> 
<li> <strong>Chongyi Li</strong>, Jichang Guo, Underwater Image Enhancement by Dehazing and Color Correction,  <strong><ud2>Journal of Electronic Imaging (JEI)</ud2></strong> (SCI, IF=0.754), 2016. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Underwater%20image%20enhancement%20by%20dehazing%20and%20color%20correction.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>
<!--<li>Bo Wang, Jichang Guo, Yan Zhang, <strong>Chongyi Li</strong>, Hierarchical Feature Concatenation-Based Kernel Sparse Representations for Image Categorization, <strong><ud2>The Visual Computer (TVC)</ud2></strong> (SCI, IFï¼š1.468), 2016.  <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/Hierarchical%20feature%20concatenation-based%20kernel%20sparse%20representations%20for%20image%20categorization.pdf" target="_blank"><font color="#1C86EE">[PDF-offical version]</font></a> </li>-->
</ol>

<!--
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Domestic Papers:</h4>

<ol>
<li>é¡¾ç¿”å…ƒ, éƒ­ç»§æ˜Œ, æé‡ä»ª, è‚–åˆ©å†›ï¼ŒåŸºäºå¯¹ç§°ä¸ç¡®å®šæ€§å’Œä¸‰è·¯äº¤äº’ä¿¡æ¯çš„ç‰¹å¾å­é›†é€‰æ‹©ç®—æ³•, å¤©æ´¥å¤§å­¦å­¦æŠ¥,2021. </li>
<li>é›å­å¶, éƒ­ç»§æ˜Œ, æé‡ä»ª, èäºæ³¨æ„åŠ›æœºåˆ¶çš„å¼±ç›‘ç£æ°´ä¸‹å›¾åƒå¢å¼ºç®—æ³•, æµ™æ±Ÿå¤§å­¦å­¦æŠ¥, 2020. </li>
<li>ä¸›æ¶¦æ°‘, å¼ ç¦¹å¢¨, å¼ æ™¨, æé‡ä»ª, èµµè€€, æ·±åº¦å­¦ä¹ é©±åŠ¨çš„æ°´ä¸‹å›¾åƒå¢å¼ºä¸å¤åŸç ”ç©¶è¿›å±•, ä¿¡å·å¤„ç†, 2020. </li>
<li>éƒ­ç»§æ˜Œ, æé‡ä»ª, éƒ­æ˜¥ä¹, æ°´ä¸‹å›¾åƒå¢å¼ºå’Œå¤åŸæ–¹æ³•ç ”ç©¶è¿›å±•ç»¼è¿°, ä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥, 2017, 3: 273-287. </li>
<li>éƒ­ç»§æ˜Œ, æé‡ä»ª, å¼ è‰³, é¢å‘æ°´ä¸‹å›¾åƒçš„è´¨é‡è¯„ä»·æ–¹æ³•, ä¸­å›½å›¾è±¡å›¾å½¢å­¦æŠ¥, 2017, 1: 1-8. </li>
<li>é¡¾ç¿”å…ƒ, éƒ­ç»§æ˜Œ, ç”°ç…œè¡¡, æé‡ä»ª, åŸºäºæ¡ä»¶äº’ä¿¡æ¯çš„ç©ºåŸŸéšå†™æ£€æµ‹ç‰¹å¾é€‰æ‹©ç®—æ³•, å¤©æ´¥å¤§å­¦å­¦æŠ¥, 2017. </li>  
<li>ç‹å»º,   å®‹å æ°, æé‡ä»ª, æ°´ä¸‹å›¾åƒå¢å¼ºæ–¹æ³•ç ”ç©¶ç°çŠ¶, æµ·æ´‹æŠ€æœ¯å­¦æŠ¥, 2016.</li>
</ol>
  -->
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Invited Talk:</h4>

<ol>
<li><strong>Comments on a paper in Nature Communications on Machine Learning</strong>. Invited by Connie Chang, SCIENTIFIC AMERICAN. <a href="https://www.scientificamerican.com/article/moons-hidden-depths-uncovered-with-new-algorithm/" target="_blank"><font color="#ff0000">[Article]</font></a>, February 2022 Issue</font></a></a></li>    
<li><strong>Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation</strong>. Invited Talk, SenseTime. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/S-Lab-v2_sensetime_talk_lichongyi.pdf" target="_blank"><font color="#ff0000">[PPT]</font></a><a href="https://www.bilibili.com/video/BV1Gv411j7HN?p=2" target="_blank"><font color="#ff0000">[Talk Video (@bilibili in Chinese AIç”»è´¨)], 23/04/2021</font></a></a></li>    
<li><strong>Underwater Image Enhancement: Challenges, Benchmarks, and Solutions</strong>. Invited Talk, Center of Digital Media Information Processing, Beijing Jiaotong University. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/BeijiaoPoster.jpg" target="_blank"><font color="#ff0000">[Poster]</font></a></a></li>  
<li><strong>ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D salient Object Detection</strong>. Invited Talk, ShenZhen Institutes of Advanced Technology, Chinese Academy of Sciences. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/talk_poster1.jpg" target="_blank"><font color="#ff0000">[Poster]</font></a></a></li>  


</ol>
  
<h4>
<a id="publications-J-pages" class="anchor" href="#publications-J-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Patent:</h4>

<ol>
  
<li> <strong>US Patent</strong> System and Method for Processing An Image<a href="https://patents.google.com/patent/US20210377468A1/en" target="_blank"><font color="#1C86EE">[Link]</font></a> <a href="/PDF/HFP08922_copy of cert.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a></li> 
<li> <strong>China Patent</strong> ZL 2016 1 0606187.9 <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/%E5%8F%91%E6%98%8E%E4%B8%93%E5%88%A9%E8%AF%81%E4%B9%A6-%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E6%9C%80%E4%BC%98%E5%8C%96%E9%A2%9C%E8%89%B2%E4%BF%AE%E6%AD%A3%E5%92%8C%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B0%B4%E4%B8%8B%E5%9B%BE%E5%83%8F%E5%A4%8D%E5%8E%9F%E6%96%B9%E6%B3%95.jpg" target="_blank"><font color="#1C86EE">[PDF]</font></a> </li>
<li> <strong>China Patent</strong> ZL 2016 1 1028224.9 <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/%E5%8F%91%E6%98%8E%E4%B8%93%E5%88%A9%E8%AF%81%E4%B9%A6-%E5%9F%BA%E4%BA%8E%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%85%88%E9%AA%8C%E4%BF%A1%E6%81%AF%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%BE%E6%96%B9%E6%B3%95.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a> </li>
<li> <strong>China Patent</strong> ZL 2016 1 0244589.9 <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/%E5%8F%91%E6%98%8E%E4%B8%93%E5%88%A9%E8%AF%81%E4%B9%A6_%E4%B8%80%E7%A7%8D%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a> </li>
<li> <strong>China Patent</strong> ZL 2018 1 0879724.6 <a href="PDF/201810879724.6.pdf" target="_blank"><font color="#1C86EE">[PDF]</font></a> </li>

  
</ol>
</div>


</section>

</div>
<script src="../javascripts/scale.fix.js"></script>
</body>
</html>
